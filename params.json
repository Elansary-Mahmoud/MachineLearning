{"name":"Machine Learning for Developers","tagline":"Literature Study","body":"Most developers these days have heard of machine learning, but when trying to find an 'easy' way into this technique, most people find themselves getting scared off by the abstractness of the concept of *Machine Learning* and terms as [*regression*](http://en.wikipedia.org/wiki/Regression_analysis), [*unsupervised learning*](http://en.wikipedia.org/wiki/Unsupervised_learning), [*Probability Density Function*](http://en.wikipedia.org/wiki/Probability_density_function) and many other definitions. If one switches to books there are books such as [*An Introduction to Statistical Learning with Applications in R*](http://www-bcf.usc.edu/~gareth/ISL/) and [*Machine Learning for Hackers*](http://shop.oreilly.com/product/0636920018483.do) who use programming language [R](http://www.r-project.org) for their examples.\r\n\r\nHowever R is not really a programming language in which one writes programs for everyday use such as is done with for example Java, C#, Scala etc. This is why in this blog machine learning will be introduced using [Smile](https://github.com/haifengl/smile), a machine learning library that can be used both in Java and Scala. These are languages that most developers have seen at least once during their study or career. \r\n\r\nThe first section **['The global idea of machine learning'](#the-global-idea-of-machine-learning)** contains all important concepts and notions you need to know about to get started with the practical examples that are described in the section ['Practical Examples'](#practical-examples). The section practical examples is inspired by the examples from the book [*Machine Learning for Hackers*](http://shop.oreilly.com/product/0636920018483.do). Additionally the book [Machine Learning in Action](http://www.manning.com/pharrington/) was used for validation purposes.\r\n\r\nThe second section **[Practical examples](#practical-examples)** contains examples for various machine learning (ML) applications, with [Smile](https://github.com/haifengl/smile) as ML library. \r\n\r\nNote that in this blog, 'new' definitions are hyperlinked such that if you want, you **can** read more regarding that specific topic, but you are not obliged to do this in order to be able to work through the examples. \r\n\r\nAs final note I'd like to thank the following people:\r\n\r\n* [Haifeng Li](https://www.linkedin.com/in/haifengli) for his support and writing the awesome and free to use library [Smile](https://github.com/haifengl/smile).\r\n* [Erik Meijer](https://www.linkedin.com/profile/view?id=1490860) for all suggestions and supervision of the process of writing this blog.\r\n* [Richard van Heest](https://www.linkedin.com/profile/view?id=138460950) for his feedback and co-reading the blog.\r\n* [Lars Willems](https://www.linkedin.com/profile/view?id=355522383) for his feedback and co-reading the blog.\r\n\r\n#The global idea of machine learning\r\nYou probably have heard about Machine learning as a concept one time or another. However, if you would have to explain what machine learning is to another person, how would you do this? Think about this for a second before reading the rest of this section.\r\n\r\nMachine learning is explained in many ways, some more accurate than others, however there is a lot of inconsistency in its definition. Some say machine learning is generating a static model based on historical data, which then allows you to predict for future data. Others say it's a dynamic model that keeps on changing as more data is added over time.\r\n\r\nI agree more with the dynamic definition but due to certain limitations we explain the static model method in the examples. However, we do explain how the dynamic principle would work in the subsection [Dynamic machine learning](#dynamic-machine-learning).\r\n\r\nThe upcoming subsections explain commonly used definitions and notions in the machine learning field. We advise you to read through these before starting the practical examples.\r\n\r\n##Features\r\nA *feature* is a property on which a [model](#model) is trained. Say for example that you classify emails as spam/ham based on the frequency of the word 'Buy' and 'Money'. Then these words are features, or part of a feature if you would combine it with more words. If you would use machine learning to predict whether one is a friend of yours, the amount of 'common' friends could be a feature. Note that in the field, sometimes features are also referred to as *attributes*.\r\n\r\n##Model\r\nWhen one talks about machine learning, often the term *model* is mentioned. The model is the result of any machine learning method and the algorithm used within this method. This model can be used to make predictions in [supervised](#supervised-learning), or to retrieve clusterings in [unsupervised learning](#unsupervised-learning). Chances are high that you will encounter the terms **online** and **offline** training of a model in the field. The idea behind online training is that you add training data to an already existing model, whereas with offline training you generate a new model from scratch. For performance reasons, online training would be the most preferable method. However for some algorithms this is not possible.\r\n\r\n##Learning methods\r\nIn the field of machine learning there are two leading ways of learning, namely [Supervised learning](#supervised-learning) and [Unsupervised learning](#unsupervised-learning). A brief introduction is necessary when you want to use Machine learning in your applications, as picking the right machine learning approach and algorithm is an important but sometimes also a little tedious process.\r\n\r\n###Supervised learning\r\nIn supervised learning you define explicitly what features you want to use, and what output you expect. An example is predicting gender based on height and weight, which is known as a [Classification](#classification) problem. Additionally you can also predict absolute values with [Regression](#regression). An example of regression with the same data would be predicting one's length based on gender and weight. Some supervised algorithms can only be used for either classification or regression, such as [K-NN](#labeling-isps-based-on-their-downupload-speed-k-nn-using-smile-in-scala). However there also exists algorithms such as [Support Vector Machines](#using-support-vector-machines-svms) which can be used for both purposes. \r\n\r\n####Classification\r\nThe problem of classification within the domain of Supervised learning is relatively simple. Given a set of labels, and some data that already received the correct labels, we want to be able to *predict* labels for new data that we did not label yet. However, before thinking of your data as a classification problem, you should look at what the data looks like. If there is a clear structure in the data such that you can easily draw a regression line it might be better to use a [regression](#regression) algorithm instead. Given the data does not fit to a regression line, or when performance becomes an issue, classification is a good alternative.\r\n\r\nAn example of a classification problem would be to classify emails as ham or spam based on their content. Given a training set in which emails are labeled ham or spam, a classification algorithm can be used to train a [Model](#model). This model can then be used to predict for future emails whether they are ham or spam. A typical example of a classification algorithm is the [K-NN algorithm](#labeling-isps-based-on-their-downupload-speed-k-nn-using-smile-in-scala). Another commonly used example of a classification problem is [Classifying email as spam or ham](#classifying-email-as-spam-or-ham-naive-bayes) which is also one of the examples written on this blog.\r\n\r\n####Regression\r\nRegression is a lot stronger in comparison to [classification](#classification). This is because in regression you are predicting actual values, rather than labels. Let us clarify this with a short example: given a table of weights, heights, and genders, you can use [K-NN](#labeling-isps-based-on-their-downupload-speed-k-nn-using-smile-in-scala) to predict one's gender when given a weight and height. With this same dataset using regression, you could instead predict one's weight or height, given the gender and the respective other missing parameter. \r\n\r\nWith this extra power, comes great responsibility, thus in the working field of regression one should be very careful when generating the model. [Common pitfalls](#common-pitfalls) are [overfitting](#overfitting), [underfitting](#underfitting) and not taking into account how the model handles [extrapolation](http://en.wikipedia.org/wiki/Extrapolation) and [interpolation](http://en.wikipedia.org/wiki/Interpolation).\r\n\r\n###Unsupervised learning\r\nIn contrast to [supervised](#supervised-learning), with [unsupervised](http://en.wikipedia.org/wiki/Unsupervised_learning) learning you do not exactly know the output on beforehand. The idea when applying unsupervised learning is to find hidden underlying structure in a dataset. An example would be [PCA](#principal-components-analysis-pca) with which you can reduce the amount of features by combining features. This combining is done based on the possibly hidden correlation between these features. Another example of unsupervised learning is [K-means clustering](#http://en.wikipedia.org/wiki/K-means_clustering). The idea behind K-means clustering is to find groups within a dataset, such that these groups can later on be used for purposes such as [supervised learning](#supervised-learning).\r\n\r\n####Principal Components Analysis (PCA)\r\nPrincipal Components Analysis is a technique used in statistics to convert a set of correlated columns into a smaller set of uncorrelated columns, reducing the amount of features of a problem. This smaller set of columns are called the principal components. This technique is mostly used in exploratory data analysis as it reveals internal structure in the data that can not be found with eye-balling the data.\r\n\r\nA big weakness of PCA however are outliers in the data. These heavily influence its result, thus looking at the data on beforehand, eliminating large outliers can greatly improve its performance.\r\n\r\nTo give a clear idea of what PCA does, we show the plots of a dataset of points with 2 dimensions in comparison to the same dataset plotted after PCA is applied.\r\n\r\n<img src=\"./Images/PCA_Explanatory_Data.png\" width=\"350px\"/>\r\n<img src=\"./Images/PCA_Reduced_Dimension.png\" width=\"330px\" />\r\n\r\nOn the left plot the original data is shown, where each color represents a different class. It is clear that from the 2 dimensions (X and Y) you could reduce to 1 dimension and still classify properly. This is where PCA comes in place. With PCA a new value is calculated for each datapoint, based on its original dimensions. \r\n\r\nIn the plot on the right you see the result of applying PCA to this data. Note that there is a y value, but this is purely to be able to plot the data and show it to you. This Y value is 0 for all values as only the X values are returned by the PCA algorithm. Also note that the values for X in the right plot do not correspond to the values in the left plot, this shows that PCA not 'just drops' a dimension.\r\n\r\n##Validation techniques\r\nIn this section we will explain some of the techniques available for model validation, and some terms that are commonly used in the Machine Learning field within the scope of validation techniques.\r\n\r\n###Cross validation\r\nThe technique of cross validation is one of the most common techniques in the field of machine learning. Its essence is to *ignore* part of your dataset while training your [model](#model), and then using the model to predict this *ignored data*. Comparing the predictions to the actual value then gives an indication of the performance of your model, and the quality of your training data.\r\n\r\nThe most important part of this cross validation is the splitting of data. You should always use the complete dataset when performing this technique. In other words you should not randomly select X datapoints for training and then randomly select X datapoints for testing, because then some datapoints can be in both sets while others might not be used at all.\r\n\r\n####(2 fold) Cross validation\r\nIn 2-fold cross validation you perform a split of the data into test and training for each fold (so 2 times) and train a model using the training dataset, followed by verification with the testing set. Doing so allows you to compute the error in the predictions for the test data 2 times. These error values then should not differ significantly. If they do, either something is wrong with your data or with the features you selected for model prediction. Either way you should look into the data more and find out what is happening for your specific case, as training a model based on the data might result in an [overfitted](#overfitting) model for erroneous data.\r\n\r\n###Regularization\r\nThe basic idea of regularization is preventing [overfitting](#overfitting) your [model](#model) by simplifying it. Suppose your data is a 3rd degree polynomial function, but your data has noise and this would cause the model to be of a higher degree. Then the model would perform poorly on new data, whereas it seems to be a good model at first. Regularization helps preventing this, by simplifying the model with a certain value *lambda*. However to find the right lambda for a model is hard when you have no idea as to when the model is overfitted or not. This is why [cross validation](#cross-validation) is often used to find the best lambda fitting your model.\r\n\r\n### Precision\r\nIn the field of computer science we use the term precision to define the amount of items selected which are actually relevant. So when you compute the precision value for a search algorithm on documents, the precision of that algorithm is defined by how many documents from the result set are actually relevant.\r\n\r\nThis value is computed as follows:\r\n\r\n<img src=\"./Images/Precision.png\"/>\r\n\r\nAs this might be a bit hard to grasp I will give an example:\r\n\r\nSay we have documents {aa, ab, bc, bd, ee} as the complete corpus, and we query for documents with `a` in the name. If our algorithm would then return the document set {aa, ab}, the precision would be 100% intuitively. Let's verify it by filling in the formula:\r\n\r\n<img src=\"./Images/PrecisionFull.png\"/>\r\n\r\nIndeed it is 100%. If we would run the query again but get more results than only {aa, ab}, say we additionally get {bc,de}  back as well, this influences the precision as follows:\r\n\r\n<img src=\"./Images/PrecisionHalf.png\"/>\r\n\r\nHere the results contained the relevant results but also 2 irrelevant results. This caused the precision to decrease. However if you would calculate the [recall](#recall) for this example it would be 100%. This is how precision and recall differ from each other.\r\n\r\n### Recall\r\nRecall is used to define the amount of relevant items that are retrieved by an algorithm given a query and a data corpus. So given a set of documents, and a query that should return a subset of those documents, the recall value represents how many of the relevant documents are actually returned. This value is computed as follows:\r\n\r\n<img src=\"./Images/Recall.png\"/>\r\n\r\nGiven this formula, let's do an example to see how it works:\r\n\r\nSay we have documents {aa,ab,bc,bd,ee} as complete corpus and we query for documents with `a` in the name. If our algorithm would be to return {aa,ab} the recall would be 100% intuitively. Let's verify it by filling in the formula:\r\n\r\n<img src=\"./Images/RecallFull.png\"/>\r\n\r\nIndeed it is 100%. Next we shall show what happens if not all relevant results are returned:\r\n\r\n<img src=\"./Images/RecallHalf.png\"/>\r\n\r\nHere the results only contained half of the relevant results. This caused the recall to decrease. However if you would compute the [precision](#precision) for this situation, it would result in 100% precision, because all results are relevant.\r\n\r\n### Prior\r\nThe prior value that belongs to a classifier given a datapoint represents the likelihood that this datapoint belongs to this classifier.  In practice this means that when you get a prediction for a datapoint, the prior value that is given with it, represents how 'convinced' the model is regarding the classification given to that datapoint. \r\n\r\n\r\n### Root Mean Squared Error (RMSE)\r\nThe Root Mean Squared Error (RMSE or RMSD where D stands for deviation) is the square root of the mean of the squared differences between the actual value and predicted value. As this is might be hard to grasp, I'll explain it using an example. Suppose we have the following values:\r\n\r\n| Predicted temperature | Actual temperature | squared difference for Model | square difference for average |\r\n| :--: | :--:| :--:| :--: | \r\n|10 | 12 | 4 | 7.1111 |\r\n|20 | 17 | 9 | 5.4444 |\r\n|15 | 15 | 0 | 0.1111 |\r\n\r\nThe mean of these squared differences for the model is 4.33333, and the root of this is 2.081666. So in average, the model predicts the values with an error of 2.08. The lower this RMSE value is, the better the model is in its predictions. This is why in the field, when selecting features, one computes the RMSE with and without a certain feature, in order to say something about how that feature affects the performance of the model. With this information one can then decide whether the additional computation time for this feature is worth it in comparison to the improvement rate on the model.\r\n\r\nAdditionally, because the RMSE is an absolute value, it can be normalised in order to compare models. This results in the Normalised Root Mean Square Error (NRMSE). For computing this however, you need to know the minimum and maximum value that the system can contain. Let's suppose we can have temperatures ranging from minimum of 5 to a maximum of 25 degrees, then computing the NRMSE is as follows:\r\n\r\n<img src=\"./Images/Formula4.png\"/>\r\n\r\nWhen we fill in the actual values we get the following result:\r\n\r\n<img src=\"./Images/Formula1.png\"/>\r\n\r\nNow what is this 10.45 value? This is the error percentage the model has in average on its datapoints.\r\n\r\nFinally we can use RMSE to compute a value that is known in the field as **R Squared**. This value represents how good the model performs in comparison to ignoring the model and just taking the average for each value. For that you need to calculate the RMSE for the average first. This is 4.22222 (taking the mean of the values from the last column in the table), and the root is then 2.054805. The first thing you should notice is that this value is lower than that of the model. This is not a good sign, because this means the model performs **worse** than just taking the mean. However to demonstrate how to compute **R Squared** we will continue the computations.\r\n\r\nWe now have the RMSE for both the model and the mean, and then computing how well the model performs in comparison to the mean is done as follows:\r\n\r\n<img src=\"./Images/Formula2.png\" />\r\n\r\nThis results in the following computation:\r\n\r\n<img src=\"./Images/Formula3.png\" />\r\n\r\nNow what does this -1.307229 represent? Basically it says that the model that predicted these values performs about 1.31 percent worse than returning the average each time a value is to be predicted. In other words, we could better use the average function as a predictor rather than the model in this specific case.\r\n\r\n##Common pitfalls \r\nThis section describes some common pitfalls in applying machine learning techniques. The idea of this section is to make you aware of these pitfalls and help you prevent actually walking into one yourself.\r\n\r\n###Overfitting\r\nWhen fitting a function on the data, there is a possibility the data contains noise (for example by measurement errors). If you fit every point from the data exactly, you incorporate this noise into the [model](#model). This causes the model to predict really well on your test data, but relatively poor on future data.\r\n\r\nThe left image here below shows how this overfitting would look like if you were to plot your data and the fitted functions, whereas the right image would represent a *good fit* of the regression line through the datapoints.\r\n\r\n<img src=\"./Images/OverFitting.png\" width=\"300px\" /> \r\n<img src=\"./Images/Good_Fit.png\" width=\"300px\" />\r\n\r\nOverfitting can easily happen when applying [regression](#regression) but can just as easily be introduced in [Naive Bayes classifications](#classifying-email-as-spam-or-ham-naive-bayes). In regression it happens with rounding, bad measurements and noisy data. In naive bayes however, it could be the features that were picked. An example for this would be classifying spam or ham while keeping all stop words.\r\n\r\nOverfitting can be detected by performing [validation techniques](#validation-techniques) and looking into your data's statistical features, and detecting and removing outliers.\r\n\r\n### Underfitting\r\nWhen you are turning your data into a model, but are leaving (a lot of) statistical data behind, this is called underfitting. This can happen due to various reasons, such as using a wrong regression type on the data. If you have a non-linear structure in the data, and you apply linear regression, this would result in an under-fitted model. The left image here below represents an under-fitted regression line whereas the right image shows a good fit regression line.\r\n\r\n<img src=\"./Images/Under-fitting.png\" width=\"300px\" /> \r\n<img src=\"./Images/Good_Fit.png\" width=\"300px\" />\r\n\r\nYou can prevent underfitting by plotting the data to get insights in the underlying structure, and using [validation techniques](#validation-techniques) such as [cross validation](#2-fold-cross-validation). \r\n\r\n###Curse of dimensionality\r\nThe curse of dimensionality is a collection of problems that can occur when your data size is lower than the amount of features (dimensions) you are trying to use to create your machine learning [model](#model). An example of a dimensionality curse is matrix rank deficiency. When using [Ordinary Least Squares(OLS)](http://en.wikipedia.org/wiki/Ordinary_least_squares), the underlying algorithm solves a linear system in order to build up a model. However if you have more columns than you have rows, coming up with a single solution for this system is not possible. If this is the case, the best solution would be to get more datapoints or reduce the feature set. \r\n\r\nIf you want to know more regarding this curse of dimensionality, there is [a study focussed on this issue](http://lectures.molgen.mpg.de/networkanalysis13/LDA_cancer_classif.pdf). In this study, researchers Haifeng Li, Keshu Zhang and Tao Jiang developed an algorithm that improves cancer classification with very few datapoints. They compared their algorithm with [support vector machines](http://en.wikipedia.org/wiki/Support_vector_machine) and [random forests](http://en.wikipedia.org/wiki/Random_forest).\r\n\r\n##Dynamic machine learning\r\nIn almost all literature you can find about machine learning, a static model is generated and validated, and then used for predictions or recommendations. However in practice, this alone would not make a very good machine learning application. This is why in this section we will explain how to turn a static model into a dynamic model. Since the (most optimal) implementation depends on the algorithm you are using, we will explain the concept rather than giving a practical example. Because explaining it in text only will not be very clear we first present you the whole system in a diagram. We will then use this diagram to  explain machine learning and how to make the system dynamic.\r\n\r\n<img src=\"./Images/DynamicMachineLearning.png\" />\r\n\r\nThe basic idea of machine learning can be described by the following steps:\r\n\r\n1. Gather data\r\n2. Split the data into a testing and training set\r\n3. Train a model (with help of a machine learning algorithm)\r\n4. Validate the model with a validation method which takes the model and testing data\r\n5. do predictions based on the model\r\n\r\nIn this process there are a few steps missing when it comes to actual applications in the field. These steps are in my opinion the most important steps to make a system actually learn.\r\n\r\nThe idea behind what we call dynamic machine learning is as follows: You take your predictions, combine it with user feedback and feed it back into your system to improve your dataset and model. As we just said we need user feedback, so how is this gained? Let's take friend suggestions on [Facebook](https://www.facebook.com) for example. The user is presented 2 options: 'Add Friend' or 'Remove'. Based on the decision of the user, you have direct feedback regarding that prediction. \r\n\r\nSo say you have this user feedback, then you can apply machine learning over your machine learning application to learn about the feedback that is given. This might sound a bit strange, but we will try to explain this more elaborately. However before we do this, we need to make a *disclaimer*: our description of the Facebook friend suggestion system is a 100% assumption and in no way confirmed by Facebook itself. Their systems are a secret to the outside world as far as we know.\r\n\r\nSay the system predicts based on the following features:\r\n\r\n1. amount of common friends\r\n2. Same hometown\r\n3. Same age\r\n\r\nThen you can compute a [prior](#prior) for every person on Facebook regarding the chance that he/she is a good suggestion to be your friend. Say you store the results of all these predictions for a period of time, then analysing this data on its own with machine learning allows you to improve your system. To elaborate on this, say most of our 'removed' suggestions had a high rating on feature 2, but relatively low on 1, then we can add weights to the prediction system such that feature 1 is more important than feature 2. This will then improve the recommendation system for us. \r\n\r\nAdditionally, the dataset grows over time, so we should keep on updating our model with the new data to make the predictions more accurate. How to do this however, depends on the size and mutation rate of your data. \r\n\r\n#Practical examples\r\nIn this section we present you a set of machine learning algorithms in a practical setting. The idea of these examples is to get you started with machine learning algorithms without an in depth explanation of the underlying algorithms. We focus purely on the functional aspect of these algorithms, how you can verify your implementation and finally try to make you aware of [common pitfalls](#common-pitfalls).\r\n\r\nThe following examples are available:\r\n\r\n* [Labeling ISP's based on their down/upload speed (K-NN)](#labeling-isps-based-on-their-downupload-speed-k-nn-using-smile-in-scala)\r\n* [Classifying email as spam or ham (Naive Bayes)](#classifying-email-as-spam-or-ham-naive-bayes)\r\n* [Ranking emails based on their content (Recommendation system)](#ranking-emails-based-on-their-content-recommendation-system)\r\n* [Predicting weight based on height (Linear Regression OLS)](#predicting-weight-based-on-height-using-ordinary-least-squares)\r\n* [An attempt at rank prediction for top selling books using text regression](#an-attempt-at-rank-prediction-for-top-selling-books-using-text-regression)\r\n* [Using unsupervised learning to merge features (PCA)](#using-unsupervised-learning-to-merge-features-pca)\r\n* [Using Support Vector Machines (SVMS)](#using-support-vector-machines-svms)\r\n\r\nFor each of these examples we used the [Smile Machine Learning](https://github.com/haifengl/smile/releases) library. We used both the `smile-core` and `smile-plot` libraries. These libraries are available on [Maven](http://search.maven.org), Gradle, Ivy, SBT and Leiningen. Information on how to add them using one of these systems can be found [here for the core](https://www.versioneye.com/java/com.github.haifengl:smile-core/1.0.2), and [here for the plotting library](https://www.versioneye.com/java/com.github.haifengl:smile-plot/1.0.2).\r\n\r\nSo before you start working through an example, I assume you made a new project in your favourite IDE, and added the `smile-core` and `smile-plot` libraries to your project. Additional libraries when used, and how to get the example data is addressed per example.\r\n\r\n\r\n##Labeling ISPs based on their down/upload speed (K-NN using Smile in Scala)\r\n\r\nThe goal of this section is to use the K-Nearest Neighbours (K-NN) Algorithm to classify download/upload speed pairs as [internet service provider (ISP)](http://en.wikipedia.org/wiki/Internet_service_provider) Alpha (represented by 0) or Beta (represented by 1). The idea behind K-NN is as follows: given a set of points that are classified, you can classify the new point by looking at its K neighbours (K being a positive integer). The idea is that you find the K-neighbours by looking at the euclidean distance between the new point and its surrounding points. For these neighbours you then look at the biggest representative class and assign that class to the new point.\r\n\r\nTo start this example you should download the [example data](./Example%20Data/KNN_Example_1.csv). Additionally you should set the path in the code snippet to where you stored this example data.\r\n\r\nThe first step is to load the CSV data file. As this is no rocket science, I provide the code for this without further explanation:\r\n\r\n```scala\r\n\r\nobject KNNExample {\r\n   def main(args: Array[String]): Unit = {\r\n    val basePath = \"/.../KNN_Example_1.csv\"\r\n    val testData = getDataFromCSV(new File(basePath))    \r\n    }\r\n    \r\n  def getDataFromCSV(file: File): (Array[Array[Double]], Array[Int]) = {\r\n    val source = scala.io.Source.fromFile(file)\r\n    val data = source\r\n    \t.getLines()\r\n    \t.drop(1)\r\n    \t.map(x => getDataFromString(x))\r\n    \t.toArray\r\n    \t\r\n    source.close()\r\n    val dataPoints = data.map(x => x._1)\r\n    val classifierArray = data.map(x => x._2)\r\n    return (dataPoints, classifierArray)        \r\n  }\r\n  \r\n  def getDataFromString(dataString: String): (Array[Double], Int) = {\r\n\r\n    //Split the comma separated value string into an array of strings\r\n    val dataArray: Array[String] = dataString.split(',')\r\n\r\n    //Extract the values from the strings\r\n    val xCoordinate: Double = dataArray(0).toDouble\r\n    val yCoordinate: Double = dataArray(1).toDouble\r\n    val classifier: Int = dataArray(2).toInt\r\n\r\n    //And return the result in a format that can later \r\n    //easily be used to feed to Smile\r\n    return (Array(xCoordinate, yCoordinate), classifier)\r\n  }\r\n}\r\n```\r\n\r\nFirst thing you might wonder now is *why is the data formatted this way*. Well, the separation between dataPoints and their label values is for easy splitting between testing and training data, and because the API expects the data this way for both executing the K-NN algorithm and plotting the data. Secondly the datapoints stored as an ```Array[Array[Double]]``` is done to support datapoints in more than just 2 dimensions.\r\n\r\nGiven the data the first thing to do next is to see what the data looks like. For this Smile provides a nice plotting library. In order to use this however, the application should be changed to a Swing application. Additionally the data should be fed to the plotting library to get a JPane with the actual plot. After changing your code it should look like this:\r\n\r\n ```scala\r\n \r\n object KNNExample extends SimpleSwingApplication {\r\n  def top = new MainFrame {\r\n    title = \"KNN Example\"\r\n    val basePath = \"/.../KNN_Example_1.csv\"\r\n\r\n    val testData = getDataFromCSV(new File(basePath))\r\n\r\n    val plot = ScatterPlot.plot(testData._1,\r\n    \t\t\t\t\t\t\t testData._2, \r\n    \t\t\t\t\t\t\t '@', \r\n    \t\t\t\t\t\t\t Array(Color.red, Color.blue)\r\n    \t\t\t\t\t\t\t)\r\n    peer.setContentPane(plot)\r\n    size = new Dimension(400, 400)\r\n\r\n  }\r\n  ...\r\n  ```\r\n \r\nThe idea behind plotting the data is to verify whether K-NN is a fitting Machine Learning algorithm for this specific set of data. In this case the data looks as follows:\r\n\r\n<img src=\"./Images/KNNPlot.png\" width=\"300px\" height=\"300px\" />\r\n\r\nIn this plot you can see that the blue and red points seem to be mixed in the area (3 < x < 5) and (5 < y < 7.5). Since the groups are mixed the K-NN algorithm is a good choice, as fitting a linear decision boundary would cause a lot of false classifications in the mixed area.\r\n\r\nGiven this choice to use the K-NN algorithm to be a good fit for this problem, let's continue with the actual Machine Learning part. For this the GUI is ditched since it does not really add any value. Recall from the section [*The global idea of Machine Learning*](#the-global-idea-of-machine-learning) that in machine learning there are 2 key parts: Prediction and Validation. First we will look at the validation, as using a model without any validation is never a good idea. The main reason to validate the model here is to prevent [overfitting](#overfitting). However, before we even can do validation, a *correct* K should be chosen. \r\n\r\nThe drawback is that there is no golden rule for finding the correct K. However, finding a *good* K that allows for most datapoints to be classified correctly can be done by looking at the data. Additionally the K should be picked carefully to prevent undecidability by the algorithm. Say for example ```K=2```, and the problem has 2 labels, then when a point is between both labels, which one should the algorithm pick. There is a *rule of thumb* that K should be the square root of the number of features (on other words the number of dimensions). In our example this would be ```K=1```, but this is not really a good idea since this would lead to higher false-classifications around decision boundaries. Picking ```K=2``` would result in the error regarding our two labels, thus picking ```K=3``` seems like a good fit for now.\r\n\r\nFor this example we do [2-fold Cross Validation](http://en.wikipedia.org/wiki/Cross-validation_(statistics)). In general 2-fold cross validation is a rather weak method of model Validation, as it splits the dataset in half and only validates twice, which still allows for overfitting, but since the dataset is only 100 points, 10-fold (which is a stronger version) does not make sense, since then there would only be 10 datapoints used for testing, which would give a skewed error rate.\r\n\r\n```scala\r\n\r\n  def main(args: Array[String]): Unit = {\r\n   val basePath = \"/.../KNN_Example_1.csv\"\r\n    val testData = getDataFromCSV(new File(basePath))\r\n\r\n    //Define the amount of rounds, in our case 2 and \r\n    //initialise the cross validation\r\n    val cv = new CrossValidation(testData._2.length, validationRounds)\r\n\r\n    val testDataWithIndices = (testData\r\n    \t\t\t\t\t\t\t._1\r\n    \t\t\t\t\t\t\t.zipWithIndex, \r\n    \t\t\t\t\t\t\ttestData\r\n    \t\t\t\t\t\t\t._2\r\n    \t\t\t\t\t\t\t.zipWithIndex)\r\n\r\n    val trainingDPSets = cv.train\r\n      .map(indexList => indexList\r\n      .map(index => testDataWithIndices\r\n      ._1.collectFirst { case (dp, `index`) => dp}.get))\r\n\r\n    val trainingClassifierSets = cv.train\r\n      .map(indexList => indexList\r\n      .map(index => testDataWithIndices\r\n      ._2.collectFirst { case (dp, `index`) => dp}.get))\r\n\r\n    val testingDPSets = cv.test\r\n      .map(indexList => indexList\r\n      .map(index => testDataWithIndices\r\n      ._1.collectFirst { case (dp, `index`) => dp}.get))\r\n\r\n    val testingClassifierSets = cv.test\r\n      .map(indexList => indexList\r\n      .map(index => testDataWithIndices\r\n      ._2.collectFirst { case (dp, `index`) => dp}.get))\r\n\r\n\r\n    val validationRoundRecords = trainingDPSets\r\n      .zipWithIndex.map(x => (\tx._1,\t\t\t\r\n      \t\t\t\t\t\t\ttrainingClassifierSets(x._2),\r\n      \t\t\t\t\t\t\ttestingDPSets(x._2),\r\n      \t\t\t\t\t\t\ttestingClassifierSets(x._2)\r\n      \t\t\t\t\t\t   )\r\n      \t\t\t\t\t)\r\n\r\n    validationRoundRecords\r\n    \t.foreach { record =>\r\n\r\n      val knn = KNN.learn(testData._1, testData._2, 3)\r\n\r\n      //And for each test data point make a prediction with the model\r\n      val predictions = record\r\n      \t._3\r\n      \t.map(x => knn.predict(x))\r\n\t\t.zipWithIndex\r\n\r\n      //Finally evaluate the predictions as correct or incorrect\r\n      //and count the amount of wrongly classified data points.\r\n      \r\n      val error = predictions\r\n      \t.map(x => if (x._1 != record._4(x._2)) 1 else 0)\r\n      \t.sum\r\n\r\n      println(\"False prediction rate: \" + error / predictions.length * 100 + \"%\")\r\n    }\r\n  }\r\n  ```\r\n  \r\nIf you execute this code several times you might notice the false prediction rate to fluctuate quite a bit. This is due to the random samples taken for training and testing. When this random sample is taken a bit unfortunate, the error rate becomes much higher while when taking a good random sample, the error rate could be extremely low. \r\n\r\nUnfortunately I cannot provide you with a golden rule to when your model was trained with the best possible training set. One would say that the model with the least error rate is always the best, but when you recall the term [overfitting](#overfitting), picking this particular model might perform really bad on future data. This is why having a large enough and representative dataset is key to a good Machine Learning application. However, when aware of this issue, you could implement manners to keep updating your model based on new data and known correct classifications.\r\n\r\nLet's recap what we've done so far. First you took care of getting the training and testing data. Next up you generated and validated several models and picked the model which gave the best results. Then we now have one final step to do, which is making predictions using this model:\r\n\r\n```scala\r\nval knn = KNN.learn(record._1, record._2, 3)\r\nval unknownDataPoint = Array(5.3, 4.3)\r\nval result = knn.predict(unknownDatapoint)\r\nif (result == 0)\r\n{\r\n\tprintln(\"Internet Service Provider Alpha\")\r\n}\r\nelse if (result == 1)\r\n{\r\n\tprintln(\"Internet Service Provider Beta\")\r\n}\r\nelse\r\n{\r\n\tprintln(\"Unexpected prediction\")\r\n}\r\n\r\n```\r\n\r\nThe result of executing this code is labeling the ```unknownDataPoint``` (5.3, 4.3) as ISP Alpha. This is one of the easier points to classify as it is clearly in the Alpha field of the datapoints in the plot. As it is now clear how to do these predictions I won't present you with other points, but feel free to try out how different points get predicted.\r\n\r\n\r\n\r\n##Classifying email as spam or ham (Naive Bayes)\r\nIn this example we will be using the [Naive Bayes algorithm](http://en.wikipedia.org/wiki/Naive_Bayes_classifier) to classify email as ham (good emails) or spam (bad emails) based on their content. The Naive Bayes algorithm calculates the probability for an object for each possible class, and then returns the class with the highest probability. For this probability calculation the algorithm uses features. The reason it's called *Naive* Bayes is because it does not incorporate any correlation between features. In other words, each feature counts the same. I'll explain a bit more using an example:\r\n\r\nSay you are classifying fruits and vegetables based on the features *color*, *diameter* and *shape* and you have the following classes: *apple*, *tomato*, and *cranberry*.\r\n\r\nSuppose you then want to classify an object with the following values for the features: (red,4 cm, round). This would obviously be a tomato for us, as it is way to small to be an apple, and too large for a cranberry. However, because the Naive Bayes algorithm evaluates each feature individually it will classify it as follows:\r\n\r\n* Apple  66.6% probable (based on color, and shape)\r\n* Tomato 100.0% probable (based on color, shape and size)\r\n* cranberry 66.6% probable (based on color and shape)\r\n\r\nThus even though it seems really obvious that it can't be a cranberry or apple, Naive Bayes still gives it a 66.6% change of being either one. So even though it classifies the tomato correctly, it can give poor results in edge cases where the size is just outside the scope of the training set. However, for spam classification Naive Bayes works well, as spam or ham cannot be classified purely based on one feature (word).\r\n\r\nAs you should now have an idea on how the Naive Bayes algorithm works, we can continue with the actual example. For this example we will use the Naive Bayes implementation from [Smile](https://github.com/haifengl/smile) in Scala to classify emails as spam or ham based on their content.  \r\n\r\nBefore we can start however, you should download the [data](http://spamassassin.apache.org/publiccorpus/) for this example from the SpamAssasins public corpus. The data you need for the example is the [easy_ham](http://spamassassin.apache.org/publiccorpus/20030228_easy_ham.tar.bz2) and [spam](http://spamassassin.apache.org/publiccorpus/20030228_spam.tar.bz2) files, but the rest can also be used in case you feel like experimenting some more. You should unzip these files and adjust the file paths in the code snippets to match the location of the folders. Additionally you will need the [stop words file](./Example%20Data/stopwords.txt) for filtering purposes. \r\n\r\nAs with every machine learning implementation, the first step is to load in the training data. However in this example we are taking it 1 step further into machine learning. In the [KNN examples](#labeling-isps-based-on-their-downupload-speed-knn-using-smile-in-scala) we had the download and upload speed as [features](#features). We did not refer to them as features, as they were the only properties available. For spam classification it is not completely trivial what to use as features. One can use the Sender, the subject, the message content, and even the time of sending as features for classifying as spam or ham.  \r\n\r\nIn this example we will use the content of the email as feature. By this we mean we will select the features (words in this case) from the bodies of the emails in the training set. In order to be able to do this, we need to build a [Term Document Matrix (TDM)](http://en.wikipedia.org/wiki/Document-term_matrix). \r\n\r\nWe will start off with writing the functions for loading the example data. This will be done with a ```getMessage``` method which gets a filtered body from an email given a File as parameter.\r\n\r\n```scala\r\n\r\ndef getMessage(file : File)  : String  =\r\n  {\r\n    //Note that the encoding of the example files is latin1,\r\n    // thus this should be passed to the fromFile method.\r\n    val source = scala.io.Source.fromFile(file)(\"latin1\")\r\n    val lines = source.getLines mkString \"\\n\"\r\n    source.close()\r\n    //Find the first line break in the email, \r\n    //as this indicates the message body\r\n    val firstLineBreak = lines.indexOf(\"\\n\\n\")\r\n    //Return the message body filtered by only text from a-z and to lower case\r\n     \r\n    return lines\r\n    \t.substring(firstLineBreak)\r\n    \t.replace(\"\\n\",\" \")\r\n    \t.replaceAll(\"[^a-zA-Z ]\",\"\")\r\n    \t.toLowerCase()\r\n  }\r\n```\r\n\r\nNow we need a method that gets all the filenames for the emails, from the example data folder structure that we provided you with.\r\n\r\n```scala\r\n  \r\n  def getFilesFromDir(path: String):List[File] = {\r\n    val d = new File(path)\r\n    if (d.exists && d.isDirectory) {\r\n      //Remove the mac os basic storage file, \r\n      //and alternatively for unix systems \"cmds\"\r\n      \r\n      d\t.listFiles\r\n      \t.filter(x => x\t.isFile &&\r\n      \t\t\t\t !x\t.toString\r\n      \t\t\t\t \t.contains(\".DS_Store\") \t&&\r\n      \t\t\t\t !x\t.toString\r\n      \t\t\t\t \t.contains(\"cmds\"))\r\n      \t\t\t\t \t.toList\r\n    } \r\n    else {\r\n      List[File]()\r\n    }\r\n  }\r\n```\r\n\r\nAnd finally let's define a set of paths that make it easier to load the different datasets from the example data. Together with this we also directly define a sample size of 500, as this is the complete amount of training emails  available for the spam set. We take the same amount of ham emails as the training set should be balanced for these two classification groups.\r\n\r\n```scala\r\n  \r\n  def main(args: Array[String]): Unit = {\r\n    val basePath = \"/Users/../Downloads/data\"\r\n    val spamPath = basePath + \"/spam\"\r\n    val spam2Path = basePath + \"/spam_2\"\r\n    val easyHamPath = basePath + \"/easy_ham\"\r\n    val easyHam2Path = basePath + \"/easy_ham_2\"\r\n\r\n  \tval amountOfSamplesPerSet = 500\r\n    val amountOfFeaturesToTake = 100   \r\n    //First get a subset of the filenames for the spam \r\n    // sample set (500 is the complete set in this case)\r\n    val listOfSpamFiles =   getFilesFromDir(spamPath)\r\n    \t.take(amountOfSamplesPerSet)\r\n    //Then get the messages that are contained in these files\r\n  \tval spamMails = listOfSpamFiles.map(x => (x, getMessage(x)))\r\n    \r\n     //Get a subset of the filenames from the ham sample set\r\n     // (note that in this case it is not necessary to randomly\r\n     // sample as the emails are already randomly ordered)\r\n     \r\n  \tval listOfHamFiles =   getFilesFromDir(easyHamPath)\r\n  \t\t.take(amountOfSamplesPerSet)\r\n  \t//Get the messages that are contained in the ham files\r\n  \tval hamMails  = listOfHamFiles\r\n  \t\t.map{x => (x,getMessage(x)) }\r\n  }\r\n  \r\n```\r\n\r\nNow that we have the training data for both the ham and the spam email, we can start building 2 [TDM's](http://en.wikipedia.org/wiki/Document-term_matrix). But before we show you the code for this, let's first explain shortly why we actually need this. The TDM will contain **ALL** words which are contained in the bodies of the training set, including frequency rate. However, since frequency might not be the best measure (as 1 email which contains 1.000.000 times the word 'cake' would mess up the complete table) we will also compute the **occurrence rate**. By this we mean, the amount of documents that contain that specific term. So let's start off with generating the two TDM's.\r\n\r\n```scala\r\n\r\n val spamTDM  = spamMails\r\n      .flatMap(email => email\r\n        ._2.split(\" \")\r\n        .filter(word => word.nonEmpty)\r\n        .map(word => (email._1.getName,word)))\r\n        .groupBy(x => x._2)\r\n        .map(x => (x._1, x._2.groupBy(x => x._1)))\r\n        .map(x => (x._1, x._2.map( y => (y._1, y._2.length))))\r\n        .toList\r\n\r\n    //Sort the words by occurrence rate descending \r\n    //(amount of times the word occurs among all documents)\r\n  val sortedSpamTDM = spamTDM\r\n  \t.sortBy(x =>  - (x._2.size.toDouble / spamMails.length))\r\n  \t\r\n  val hamTDM  = hamMails\r\n      .flatMap(email => email\r\n      ._2.split(\" \")\r\n      .filter(word => word.nonEmpty)\r\n      .map(word => (email._1.getName,word)))\r\n      .groupBy(x => x._2)\r\n      .map(x => (x._1, x._2.groupBy(x => x._1)))\r\n      .map(x => (x._1, x._2.map( y => (y._1, y._2.length))))\r\n      .toList\r\n\r\n    //Sort the words by occurrence rate  descending \r\n    //(amount of times the word occurs among all documents)\r\n    val sortedHamTDM = hamTDM\r\n    \t.sortBy(x =>  - (x._2.size.toDouble / spamMails.length))\r\n\r\n```\r\n\r\nGiven the tables, I've generated images using a [wordcloud](http://www.wordle.net/) for some more insight. Let's take a look at the top 50 words for each table as represented in these images. Note that the red words are from the spam table and the green words are from the ham table. Additionally, the size of the words represents the occurrence rate. Thus the larger the word, the more documents contained that word at least once.\r\n\r\n<img src=\"./Images/Ham_Stopwords.png\" width=\"400px\" height=\"200px\" />\r\n<img src=\"./Images/Spam_Stopwords.png\" width=\"400px\" height=\"200px\" />\r\n\r\nAs you can see, mostly stop words come forward. These stop words are noise, which we should prevent as much as possible in our feature selection. Thus we should remove these from the tables before selecting the features. We've included a list of stop words in the example dataset. Let's first define the code to get these stop words.\r\n\r\n```scala\r\n  def getStopWords() : List[String] =\r\n  {\r\n    val source = scala.io.Source\r\n    \t.fromFile(new File(\"/Users/.../.../Example Data/stopwords.txt\"))(\"latin1\")\r\n    val lines = source.mkString.split(\"\\n\")\r\n    source.close()\r\n    return  lines.toList\r\n  }\r\n \r\n```\r\n\r\nNow we can expand the TDM generation code with removing the stop words from the intermediate results:\r\n\r\n```scala\r\n\r\nval stopWords = getStopWords\r\n\r\nval spamTDM  = spamMails\r\n      .flatMap(email => email\r\n      ._2.split(\" \")\r\n      .filter(word => word.nonEmpty && !stopWords.contains(word))\r\n      .map(word => (email._1.getName,word)))\r\n      .groupBy(x => x._2)\r\n      .map(x => (x._1, x._2.groupBy(x => x._1)))\r\n      .map(x => (x._1, x._2.map( y => (y._1, y._2.length))))\r\n      .toList\r\n\r\n\r\n val hamTDM  = hamMails\r\n      .flatMap(email => email\r\n      ._2.split(\" \")\r\n      .filter(word => word.nonEmpty && !stopWords.contains(word))\r\n      .map(word => (email._1.getName,word)))\r\n      .groupBy(x => x._2)\r\n      .map(x => (x._1, x._2.groupBy(x => x._1)))\r\n      .map(x => (x._1, x._2.map( y => (y._1, y._2.length))))\r\n      .toList\r\n\r\n```\r\n\r\nIf we once again look at the top 50 words for spam and ham, we see that most of the stop words are gone. We could fine-tune more, but for now let's go with this.\r\n\r\n<img src=\"./Images/Ham_No_Stopwords.png\" width=\"400px\" height=\"200px\" />\r\n<img src=\"./Images/Spam_No_Stopwords.png\" width=\"400px\" height=\"200px\" />\r\n\r\nWith this insight in what 'spammy' words and what typical 'ham-words' are, we can decide on building a feature-set which we can then use in the Naive Bayes algorithm for creating the classifier. Note: it is always better to include **more** features, however performance might become an issue when having all words as features. This is why in the field, developers tend to drop features that do not have a significant impact, purely for performance reasons. Alternatively machine learning is done running complete [Hadoop](http://hadoop.apache.org/) clusters, but explaining this would be outside the scope of this blog.\r\n\r\nFor now we will select the top 100 spammy words based on occurrence (thus not frequency) and do the same for ham words and combine this into 1 set of words which we can feed into the Bayes algorithm. Finally we also convert the training data to fit the input of the Bayes algorithm. Note that the final feature set thus is 200 - (#intersecting words *2). Feel free to experiment with higher and lower feature counts.\r\n\r\n```scala\r\n\r\n//Add the code for getting the TDM data and combining it into a feature bag.\r\nval hamFeatures = hamTDM\r\n\t.records\r\n\t.take(amountOfFeaturesToTake)\r\n\t.map(x => x.term)\r\n\t\r\nval spamFeatures = spamTDM\r\n\t.records\r\n\t.take(amountOfFeaturesToTake)\r\n\t.map(x => x.term)\r\n\r\n//Now we have a set of ham and spam features,\r\n// we group them and then remove the intersecting features, as these are noise.\r\nvar data = (hamFeatures ++ spamFeatures).toSet\r\nhamFeatures\r\n\t.intersect(spamFeatures)\r\n\t.foreach(x => data = (data - x))\r\n\r\n//Initialise a bag of words that takes the top x features \r\n//from both spam and ham and combines them\r\nvar bag = new Bag[String] (data.toArray)\r\n//Initialise the classifier array with first a set of 0(spam)\r\n//and then a set of 1(ham) values that represent the emails\r\n\r\nvar classifiers =  \tArray.fill[Int](amountOfSamplesPerSet)(0) ++  \r\n\t\t\t\t\tArray.fill[Int](amountOfSamplesPerSet)(1)\r\n\r\n//Get the trainingData in the right format for the spam mails\r\nvar spamData = spamMails\r\n\t.map(x => bag.feature(x._2.split(\" \")))\r\n\t.toArray\r\n\r\n//Get the trainingData in the right format for the ham mails\r\nvar hamData = hamMails\r\n\t.map(x => bag.feature(x._2.split(\" \")))\r\n\t.toArray\r\n\r\n//Combine the training data from both categories\r\nvar trainingData = spamData ++ hamData\r\n```\r\n\r\nGiven this feature bag, and a set of training data, we can start training the algorithm. For this we can choose a few different models: `General`, `Multinomial` and `Bernoulli`. The `General` model needs to have a distribution defined, which we do not know on beforehand, so this is not a good option. The difference between the `Multinomial` and `Bernoulli` is the way in which they handle occurrence of words. The `Bernoulli` model only verifies whether a feature is there  (binary 1 or 0), thus leaves out the statistical data of occurrences, where as the `Multinomial` model incorporates the occurrences (represented by the value). This causes the `Bernoulli` model to perform bad on longer documents in comparison to the `Multinomial` model. Since we will be rating emails, and we want to use the occurrence, we focus on the multinomial but feel free to try out the `Bernoulli` model as well.\r\n\r\n```scala\r\n//Create the bayes model as a multinomial with 2 classification\r\n// groups and the amount of features passed in the constructor.\r\n\r\n  var bayes = new NaiveBayes(NaiveBayes.Model.MULTINOMIAL, 2, data.size)\r\n  //Now train the bayes instance with the training data,\r\n  // which is represented in a specific format due to the \r\n  //bag.feature method, and the known classifiers.\r\n\r\n  bayes.learn(trainingData, classifiers)\r\n```\r\n\r\nNow that we have the trained model, we can once again do some validation. However, in the example data we already made a separation between easy and hard ham, and spam, thus we will not apply the cross validation, but rather validate the model using these test sets. We will start with validation of spam classification. For this we use the 1397 spam emails from the spam2 folder.\r\n\r\n```scala\r\nval listOfSpam2Files =   getFilesFromDir(spam2Path)\r\nval spam2Mails = listOfSpam2Files\r\n\t.map{x => (x,getMessage(x)) }\r\n\t\r\nval spam2FeatureVectors = spam2Mails\r\n\t.map(x => bag.feature(x._2.split(\" \")))\r\n\t\r\nval spam2ClassificationResults = spam2FeatureVectors\r\n\t.map(x => bayes.predict(x))\r\n\r\n//Correct classifications are those who resulted in a spam classification (0)\r\nval correctClassifications = spam2ClassificationResults\r\n\t.count( x=> x == 0)\r\n\t\r\nprintln\t(\tcorrectClassifications + \r\n\t\t\t\" of \" + \r\n\t\t\tlistOfSpam2Files.length + \r\n\t\t\t\"were correctly classified\"\r\n\t\t)\r\n\t\t\r\nprintln\t((\t(correctClassifications.toDouble / \r\n\t\t\t listOfSpam2Files.length) * 100)  +\r\n\t\t\t  \"% was correctly classified\"\r\n\t\t)\r\n\r\n//In case the algorithm could not decide which category the email \r\n//belongs to, it gives a -1 (unknown) rather than a 0 (spam) or 1 (ham)\r\n\r\nval unknownClassifications = spam2ClassificationResults\r\n\t.count( x=> x == -1)\r\n\t\r\nprintln(\tunknownClassifications + \r\n\t\t\t\" of \" + \r\n\t\t\tlistOfSpam2Files.length +\r\n\t\t\t\"were unknowingly classified\"\r\n\t\t)\r\n\t\t\r\nprintln(\t(\t(unknownClassifications.toDouble / \r\n\t\t\t\tlistOfSpam2Files.length) * 100)  + \r\n\t\t\t\t% was unknowingly classified\"\r\n\t\t)\r\n\r\n```\r\n\r\nIf we run this code several times with different feature amounts, we get the following results:\r\n\r\n| amountOfFeaturesToTake\t| Spam (Correct)| Unknown| Ham | \r\n| ---------------------\t\t|:-------------\t| :----- |:----|\r\n| 50      \t\t\t\t\t| 1281 (91.70%)\t| 16 (1.15%)\t| 100 (7.15%) |\r\n| 100     \t\t\t\t\t| 1189 (85.11%)\t| 18 (1.29%)\t| 190 (13.6%)|\r\n| 200     \t\t\t\t\t| 1197 (85.68%)\t| 16 (1.15%)\t| 184 (13.17%)|\r\n| 400     \t\t\t\t\t| 1219 (87.26%)\t| 13 (0.93%)\t| 165 (11.81%)|\r\n\r\nNote that the amount of emails classified as Spam are the ones that are correctly classified by the model. Interestingly enough, the algorithm works best for classifying spam with only 50 features. However, recall that there were still *stop words* in the top 50 classification terms which could explain this result. If you look at how the values change as the amount of features increase (starting at 100), you can see that with more features, the overall result increases. Note that there is a group of unknown emails. For these emails the [prior](#prior) was equal for both classes. Note that this also is the case if there are no feature words for ham nor spam in the email, because then the algorithm would classify it 50% ham and 50% spam.\r\n\r\nWe will now do the same classification process for the ham emails. This is done by  changing the path from the variable ```listOfSpam2Files``` to ```easyHam2Path``` and rerunning the code. This gives us the following results:\r\n\r\n| amountOfFeaturesToTake\t| Spam | Unknown| Ham  (Correct) | \r\n| ---------------------\t\t|:-------------\t| :----- |:----|\r\n| 50      \t\t\t\t\t| 120 (8.57%)\t| 28 ( 2.0%)\t| 1252 (89.43%)\t|\r\n| 100   \t\t\t\t\t| 44 (3.14%)\t| 11 (0.79%)\t| 1345 (96.07%)\t|\r\n| 200 \t\t\t\t\t\t| 36 (2.57%)\t| 7 (0.5%)\t| 1357 (96.93%)\t|\r\n| 400     \t\t\t\t\t| 24 (1.71%)\t| 7 (0.5%)\t| 1369 (97.79%) |\r\n\r\nNote that now the correctly classified emails are those who are classified as ham. Here we see that indeed, when you use only 50 features, the amount of ham that gets classified correctly is significantly lower in comparison to the correct classifications when using 100 features. You should be aware of this and always verify your model for all classes, so in this case for both spam and ham  test data.  \r\n\r\nTo recap the example, we've worked through how you can use Naive Bayes to classify email as ham or spam, and got results of up to 87.26% correct classification for spam and 97.79% for ham. This shows that Naive Bayes indeed performs pretty well for classifying email as ham or spam. \r\n\r\nWith this we end the example of Naive Bayes. If you want to play around a bit more with Naive Bayes and Spam classification the corpus website also has a set of 'hard ham'  emails that you could try to classify correctly by tweaking the feature amounts and removing more stopwords.\r\n\r\n##Ranking emails based on their content (Recommendation system)\r\nThis example will be completely about building your own recommendation system. We will be ranking emails based on the following features: 'sender', 'subject', 'common terms in subject' and 'common terms in email body'. Later on in the example we will explain each of these features. Note that these features are for you to be defined when you make your own recommendation system. When building your own recommendation system this is one of the hardest parts. Coming up with good features is not trivial, and when you finally selected features the data might not be directly usable for these features.\r\n\r\nThe main idea behind this example is to show you how to do this feature selection, and how to solve issues that occur when you start doing this with your own data.\r\n\r\nWe will use a subset of the email data which we used in the example [Classifying email as spam or ham](#classifying-email-as-spam-or-ham-naive-bayes). This subset can be downloaded [here](http://spamassassin.apache.org/publiccorpus/20030228_easy_ham.tar.bz2). Additionally you need the [stop words file](./Example%20Data/stopwords.txt). Note that the data is a set of received emails, thus we lack half of the data, namely the outgoing emails of this mailbox. However even without this information we can do some pretty nice ranking as we will see later on.\r\n\r\nBefore we can do anything regarding the ranking system, we first need to extract as much data as we can from our email set. Since the data is a bit tedious in its format we provide the code to do this. The inline comments explain why things are done the way they are. Note that the application is a swing application with a GUI from the start. We do this because we will need to plot data in order to gain insight later on. Also note that we directly made a split in testing and training data such that we can later on test our model.\r\n\r\n```scala\r\nimport java.awt.{Rectangle}\r\nimport java.io.File\r\nimport java.text.SimpleDateFormat\r\nimport java.util.Date\r\nimport smile.plot.BarPlot\r\n\r\nimport scala.swing.{MainFrame, SimpleSwingApplication}\r\nimport scala.util.Try\r\n\r\nobject RecommendationSystem extends SimpleSwingApplication {\r\n\r\n case class EmailData(emailDate : Date, sender : String, subject : String, body : String)\r\n \r\n  def top = new MainFrame {\r\n    title = \"Recommendation System Example\"\r\n\r\n    val basePath = \"/Users/../data\"\r\n    val easyHamPath = basePath + \"/easy_ham\"\r\n\r\n    val mails = getFilesFromDir(easyHamPath).map(x => getFullEmail(x))\r\n    val timeSortedMails = mails\r\n      .map\t(x => EmailData\t(\tgetDateFromEmail(x),\r\n      \t\t\t\t\t\t\tgetSenderFromEmail(x), \r\n      \t\t\t\t\t\t\tgetSubjectFromEmail(x), \r\n      \t\t\t\t\t\t\tgetMessageBodyFromEmail(x)\r\n      \t\t\t\t\t\t)\r\n      \t\t)\r\n      .sortBy(x => x.emailDate)\r\n\r\n    val (trainingData, testingData) = timeSortedMails\r\n      \t\t.splitAt(timeSortedMails.length / 2)\r\n      \r\n      }\r\n\r\n  def getFilesFromDir(path: String): List[File] = {\r\n    val d = new File(path)\r\n    if (d.exists && d.isDirectory) {\r\n      //Remove the mac os basic storage file, \r\n      //and alternatively for unix systems \"cmds\"\r\n      d.listFiles.filter(x => \tx.isFile &&\r\n      \t\t\t\t\t \t\t!x.toString.contains(\".DS_Store\") && \r\n      \t\t\t\t\t \t\t!x.toString.contains(\"cmds\")).toList\r\n    } else {\r\n      List[File]()\r\n    }\r\n  }\r\n\r\n  def getFullEmail(file: File): String = {\r\n    //Note that the encoding of the example files is latin1, \r\n    //thus this should be passed to the from file method.\r\n    val source = scala.io.Source.fromFile(file)(\"latin1\")\r\n    val fullEmail = source.getLines mkString \"\\n\"\r\n    source.close()\r\n\r\n    fullEmail\r\n  }\r\n\r\n  def getSubjectFromEmail(email: String): String = {\r\n\r\n    //Find the index of the end of the subject line\r\n    val subjectIndex = email.indexOf(\"Subject:\")\r\n    val endOfSubjectIndex = email\t\r\n    .substring(subjectIndex)\t\t\t\t\t\t.indexOf('\\n') + subjectIndex\r\n\r\n    //Extract the subject: start of subject + 7 \r\n    // (length of Subject:) until the end of the line.\r\n    val subject = email\r\n    .substring(subjectIndex + 8, endOfSubjectIndex)\r\n    .trim\r\n    .toLowerCase\r\n\r\n    //Additionally, we check whether the email was a response and \r\n    //remove the 're: ' tag, to make grouping on topic easier:\r\n    subject.replace(\"re: \", \"\")\r\n  }\r\n\r\n  def getMessageBodyFromEmail(email: String): String = {\r\n\r\n    val firstLineBreak = email.indexOf(\"\\n\\n\")\r\n    //Return the message body filtered by only text \r\n    //from a-z and to lower case\r\n    email.substring(firstLineBreak)\r\n    .replace(\"\\n\", \" \")\r\n    .replaceAll(\"[^a-zA-Z ]\", \"\")\r\n    .toLowerCase\r\n  }\r\n\r\n  def getSenderFromEmail(email: String): String = {\r\n    //Find the index of the From: line\r\n    val fromLineIndex = email\r\n    .indexOf(\"From:\")\r\n    \r\n    val endOfLine = email\r\n    .substring(fromLineIndex)\r\n    .indexOf('\\n') + fromLineIndex\r\n\r\n    //Search for the <> tags in this line, as if they are there,\r\n    // the email address is contained inside these tags\r\n    \r\n    val mailAddressStartIndex = email\r\n    .substring(fromLineIndex, endOfLine)\r\n    .indexOf('<') + fromLineIndex + 1\r\n    \r\n    val mailAddressEndIndex = email\r\n    .substring(fromLineIndex, endOfLine)\r\n    .indexOf('>') + fromLineIndex\r\n\r\n    if (mailAddressStartIndex > mailAddressEndIndex) {\r\n\r\n      //The email address was not embedded in <> tags,\r\n      // extract the substring without extra spacing and to lower case\r\n      var emailString = email\r\n      .substring(fromLineIndex + 5, endOfLine)\r\n      .trim\r\n      .toLowerCase\r\n\r\n      //Remove a possible name embedded in () at the end of the line,\r\n      //for example in test@test.com (tester) the name would be removed here\r\n      val additionalNameStartIndex = emailString.indexOf('(')\r\n      if (additionalNameStartIndex == -1) {\r\n        emailString\r\n        .toLowerCase\r\n      }\r\n      else {\r\n        emailString\r\n        .substring(0, additionalNameStartIndex)\r\n        .trim\r\n        .toLowerCase\r\n      }\r\n    }\r\n    else {\r\n      //Extract the email address from the tags. \r\n      //If these <> tags are there, there is no () with a name in\r\n      // the From: string in our data\r\n      email\r\n      .substring(mailAddressStartIndex, mailAddressEndIndex)\r\n      .trim\r\n      .toLowerCase\r\n    }\r\n  }\r\n\r\n  def getDateFromEmail(email: String): Date = {\r\n    //Find the index of the Date: line in the complete email\r\n    val dateLineIndex = email\r\n    .indexOf(\"Date:\")\r\n    \r\n    val endOfDateLine = email\r\n    .substring(dateLineIndex)\r\n    .indexOf('\\n') + dateLineIndex\r\n\r\n    //All possible date patterns in the emails.\r\n    val datePatterns = Array(\t\"EEE MMM dd HH:mm:ss yyyy\",\r\n     \t\t\t\t\t\t\t\"EEE, dd MMM yyyy HH:mm\", \r\n    \t\t\t\t\t\t\t\"dd MMM yyyy HH:mm:ss\", \r\n     \t\t\t\t\t\t\t\"EEE MMM dd yyyy HH:mm\")\r\n\r\n    datePatterns.foreach { x =>\r\n      //Try to directly return a date from the formatting.\r\n      //when it fails on a pattern it continues with the next one\r\n      // until one works\r\n      \r\n      Try(return new SimpleDateFormat(x)\r\n      \t\t\t\t.parse(email\r\n      \t\t\t\t\t\t.substring(dateLineIndex + 5, endOfDateLine)\r\n      \t\t\t\t\t\t.trim.substring(0, x.length)))\r\n    }\r\n    //Finally, if all failed return null \r\n    //(this will not happen with our example data but without \r\n    //this return the code will not compile)\r\n    null\r\n  }\r\n}\r\n\r\n```\r\n\r\nThis pre-processing of the data is very common and can be a real pain when your data is not standardised such as with the dates and senders of these emails. However given this chunk of code we now have available the following properties of our example data: full email, receiving date, sender, subject, and body. This allows us to continue working on the actual features to use in the recommendation system. \r\n\r\nThe first recommendation feature we will make is based on the sender of the email. Those who you receive more emails from should be ranked higher than ones you get less email from. This is a strong assumption, but instinctively you will probably agree, given the fact that spam is left out. Let's look at the distribution of senders over the complete email set.\r\n\r\n```scala\r\n\r\n//Add to the top body:\r\n\r\n//First we group the emails by Sender, then we extract only the sender address \r\n//and amount of emails, and finally we sort them on amounts ascending\r\nval mailsGroupedBySender = trainingData\r\n.groupBy(x => x.sender)\r\n.map(x => (x._1, x._2.length))\r\n.toArray\r\n.sortBy(x => x._2)\r\n\r\n//In order to plot the data we split the values from the addresses as \r\n//this is how the plotting library accepts the data.\r\nval senderDescriptions = mailsGroupedBySender\r\n\t.map(x => x._1)\r\n\t\r\nval senderValues = mailsGroupedBySender\r\n\t.map(x => x._2.toDouble)\r\n\r\nval barPlot = BarPlot.plot(\"\", senderValues, senderDescriptions)\r\n   \r\n//Rotate the email addresses by -80 degrees such that we can read them\r\nbarPlot.getAxis(0).setRotation(-1.3962634)\r\nbarPlot.setAxisLabel(0, \"\")\r\nbarPlot.setAxisLabel(1, \"Amount of emails received \")\r\npeer.setContentPane(barPlot)\r\n\r\nbounds = new Rectangle(800, 600)\r\n\r\n```\r\n\r\n<img src=\"./Images/Mail_per_Sender_Distribution.png\" width=\"600px\" /> \r\n\r\nHere you can see that the most frequent sender sent 45 emails, followed by 37 emails and then it goes down rapidly. Due to these 'huge' outliers, directly using this data would result in the highest 1 or 2 senders to be rated as very important whereas the rest would not be considered in the recommendation system. In order to prevent this behaviour we will re-scale the data  by taking ```log1p```.  The ```log1p``` function takes the log of the value but on beforehand adds 1 to the value. This addition of 1 is to prevent trouble when taking the log value for senders who sent only 1 email. After taking the log the data looks like this.\r\n\r\n```scala\r\n\r\n//Code changes:\r\nval mailsGroupedBySender = trainingData\r\n\t.groupBy(x => x.sender)\r\n\t.map(x => (x._1, Math.log1p(x._2.length)))\r\n\t.toArray\r\n\t.sortBy(x => x._2)\r\n\r\nbarPlot.setAxisLabel(1, \"Amount of emails received on log Scale \")\r\n\r\n```\r\n\r\n<img src=\"./Images/Mail_per_Sender_log_Distribution.png\" width=\"400px\" />\r\n\r\nEffectively the data is still the same, however it is represented on a different scale.\r\nNotice here that the numeric values now range between 0.69 and 3.83. This range is much smaller, causing the outliers to not skew away the rest of the data. This data manipulation trick is very common in the field of machine learning. Finding the right scale requires some insight. This is why using the plotting library of [Smile](https://github.com/haifengl/smile) to make several plots on different scales can help a lot when performing this rescaling.\r\n\r\nThe next feature we will work on is the frequency and timeframe in which subjects occur. If a subject occurs more it is likely to be of higher importance. Additionally we take into account the timespan of the thread. So the frequency of a subject will be normalised with the timespan of the emails of this subject. This makes highly active email threads come up on top. Again this is an assumption we make on which emails should be ranked higher.\r\n\r\nLet's have a look at the subjects and their occurrence counts:\r\n\r\n```scala\r\n//Add to 'def top'  \r\nval mailsGroupedByThread = trainingData\r\n\t.groupBy(x => x.subject)\r\n\r\n//Create a list of tuples with (subject, list of emails)\r\nval threadBarPlotData = mailsGroupedByThread\r\n\t.map(x => (x._1, x._2.length))\r\n\t.toArray\r\n\t.sortBy(x => x._2)\r\n\t\r\nval threadDescriptions = threadBarPlotData\r\n\t.map(x => x._1)\r\nval threadValues = threadBarPlotData\r\n\t.map(x => x._2.toDouble)\r\n\r\n//Code changes in 'def top'\r\nval barPlot = BarPlot.plot(threadValues, threadDescriptions)\r\nbarPlot.setAxisLabel(1, \"Amount of emails per subject\")\r\n\r\n```\r\n\r\n<img src=\"./Images/Mail_per_Subject_Distribution.png\" width=\"400px\" />\r\n\r\nWe see a similar distribution as with the senders, so let's apply the `log1p` once more.\r\n\r\n```scala\r\n\r\n//Code change:\r\nval threadBarPlotData = mailsGroupedByThread\r\n\t.map(x => (x._1, Math.log1p(x._2.length)))\r\n\t.toArray\r\n\t.sortBy(x => x._2)\r\n```\r\n<img src=\"./Images/Mail_per_Subject_log_Distribution.png\" width=\"400px\" />\r\n\r\nHere the value's now range between 0.69 and 3.41, which is a lot better than a range of 1 to 29 for the recommendation system. However we did not incorporate the time frame yet, thus we go back to the normal frequency and apply the transformation later on. To be able to do this, we first need to get the time between the first and last thread:\r\n\r\n```scala\r\n//Create a list of tuples with (subject, list of emails, \r\n//time difference between first and last email)\r\nval mailGroupsWithMinMaxDates = mailsGroupedByThread\r\n .map(x => (x._1, x._2,\r\n    \t\t (x._2\r\n    \t\t\t.maxBy(x => x.emailDate)\r\n    \t\t\t.emailDate.getTime - \r\n    \t\t x._2\r\n    \t\t \t.minBy(x => x.emailDate)\r\n    \t\t \t.emailDate.getTime\r\n    \t\t\t\t\t\t) / 1000\r\n    \t\t  )\r\n      )\r\n\r\n//turn into a list of tuples with (topic, list of emails, \r\n// time difference, and weight) filtered that only threads occur\r\nval threadGroupedWithWeights = mailGroupsWithMinMaxDates\r\n .filter(x => x._3 != 0)\r\n .map(x => (x._1, x._2, x._3, 10 + \r\n    \t\t\tMath.log10(x._2.length.toDouble / x._3)))\r\n .toArray\r\n .sortBy(x => x._4)\r\n \r\nval threadGroupValues = threadGroupedWithWeights\r\n .map(x => x._4)\r\nval threadGroupDescriptions = threadGroupedWithWeights\r\n .map(x => x._1)\r\n    \r\n//Change the bar plot code to plot this data:\r\nval barPlot = BarPlot.plot(threadGroupValues, threadGroupDescriptions)\r\nbarPlot.setAxisLabel(1, \"Weighted amount of emails per subject\")    \r\n\r\n```\r\n\r\nNote how we determine the difference between the min and the max, and divide it by 1000. This is to scale the time value from milliseconds to seconds. Additionally we compute the weights by taking the frequency of a subject  and dividing it by the time difference. Since this value is very small, we want to rescale it up a little, which is done by taking the ```10log```. This however causes our values to become negative, which is why we add a basic value of 10 to make every value positive. The end result of this weighting is as follows:\r\n\r\n<img src=\"./Images/Weighted_Subject_Distribution.png\" width=\"400px\" />\r\n\r\nWe see our values ranging roughly between (4.4 and 8.6) which shows that outliers do not largely influence the feature anymore. Additionally we will look at the top 10 vs bottom 10 weights to get some more insight in what happened.\r\n\r\n**Top 10 weights:**\r\n\r\n| Subject \t| Frequency\t| Time frame (seconds) \t| Weight |\r\n| :-- \t\t| :-- \t\t| :-- \t\t\t| :-- | \r\n|[ilug] what howtos for soho system | 2 | 60  | 8.52 |\r\n|[zzzzteana] the new steve earle  | 2  | 120 | 8.22 | \r\n| [ilug] looking for a file / directory in zip file | 2 | 240 | 7.92 |\r\n| ouch... [bebergflame]| 2 |300  | 7.82|\r\n| [ilug] serial number in hosts file | 2 | 420 |  7.685 |\r\n| [ilug] email list management howto | 3 | 720 |  7.62 | \r\n| should mplayer be build with win32 codecs?  | 2 | 660 |  7.482 |\r\n| [spambayes] all cap or cap word subjects  | 2 | 670 |  7.48 |\r\n| [zzzzteana] save the planet, kill the people  | 3 | 1020 |  7.47 |\r\n| [ilug] got me a crappy laptop  | 2 | 720 | 7.44 |\r\n\r\n**Bottom 10 weights:**\r\n\r\n| Subject \t| Frequency\t| Time frame (seconds) \t| Weight |\r\n| :-- \t\t| :-- \t\t| :-- \t\t\t| :-- | \r\n|secure sofware key | 14  | 1822200 | 4.89 | \r\n|[satalk] help with postfix + spamassassin | 2  | 264480 | 4.88 | \r\n|<nettime> the war prayer | 2  | 301800 | 4.82 | \r\n|gecko adhesion finally sussed. | 5  | 767287 | 4.81 | \r\n|the mime information you requested (last changed 3154 feb 14) | 3  | 504420 | 4.776 | \r\n|use of base image / delta image for automated recovery from | 5  | 1405800 | 4.55 | \r\n|sprint delivers the next big thing?? | 5  | 1415280 | 4.55 | \r\n|[razor-users] collision of hashes? | 4  | 1230420 | 4.51 | \r\n|[ilug] modem problems | 2  | 709500 | 4.45 | \r\n|tech's major decline | 2  | 747660 | 4.43 | \r\n\r\nAs you can see the highest weights are given to emails which almost instantly got a follow up email response, whereas the lowest weights are given to emails with very long timeframes. This allows for emails with a very low frequency to still be rated as very important based on the timeframe in which they were sent. With this we have 2 features: the amount of emails from a sender ```mailsGroupedBySender```, and the weight of emails that belong to an existing thread ```threadGroupedWithWeights```.\r\n\r\nLet's continue with the next feature, as we want to base our ranking values on as many features as possible. This next feature will be based on the weight ranking that we just computed. The idea is that new emails with different subjects will arrive. However, chances are that they contain keywords that are similar to earlier received important subjects. This will allow us to rank emails as important before a thread (multiple messages with the same subject) was started. For that we specify the weight of the keywords to be the weight of the subject in which the term occurred. If this term occurred in multiple threads, we take the highest weight as the leading one.\r\n\r\nThere is one issue with this feature, which are stop words. Luckily we have a stop words file that allows us to remove (most) english stop words for now. However, when designing your own system you should take into account that multiple languages can occur, thus you should remove stop words for all languages that can occur in the system. Additionally you might need to be careful with removing stop words from different languages, as some words may have different meanings among the different languages. As for now we stick with removing English stop words. The code for this feature is as follows:\r\n\r\n```scala\r\n\r\n  def getStopWords: List[String] = {\r\n    val source = scala.io.Source\r\n    \t.fromFile(new File(\"/Users/../stopwords.txt\"))(\"latin1\")\r\n    val lines = source.mkString.split(\"\\n\")\r\n    source.close()\r\n    lines.toList\r\n   }\r\n\r\n//Add to top:\r\nval stopWords = getStopWords\r\n\r\nval threadTermWeights =  threadGroupedWithWeights\r\n\t.toArray\r\n\t.sortBy(x => x._4)\r\n\t.flatMap(x => x._1\r\n\t\t\t.replaceAll(\"[^a-zA-Z ]\", \"\")\r\n\t\t\t.toLowerCase.split(\" \")\r\n\t\t\t.filter(_.nonEmpty)\r\n\t\t\t.map(y => (y,x._4)))\r\n\t\t\t\r\nval filteredThreadTermWeights = threadTermWeights\r\n\t.groupBy(x => x._1)\r\n\t.map(x => (x._1, x._2.maxBy(y => y._2)._2))\r\n\t.toArray.sortBy(x => x._1)\r\n\t.filter(x => !stopWords.contains(x._1))\r\n\r\n```\r\n\r\nGiven this code we now have a list ```filteredThreadTermWeights``` of terms including weights that are based on existing threads. These weights can be used to compute the weight of the subject of a new email even if the email is not a response to an existing thread.\r\n\r\nAs fourth feature we want to incorporate weighting based on the terms that are occurring with a high frequency in all the emails. For this we build up a [TDM](http://en.wikipedia.org/wiki/Document-term_matrix), but this time the TDM is a bit different as in the former examples, as we only log the frequency of the terms in all documents. Furthermore we take the log10 of the occurrence counts. This allows us to scale down the term frequencies such that the results do not get affected by possible outlier values.\r\n\r\n```scala\r\n\r\nval tdm = trainingData\r\n      .flatMap(x => x.body.split(\" \"))\r\n      .filter(x => x.nonEmpty && !stopWords.contains(x))\r\n      .groupBy(x => x)\r\n      .map(x => (x._1, Math.log10(x._2.length + 1)))\r\n      .filter(x => x._2 != 0)\r\n\r\n```\r\n\r\nThis ```tdm``` list  allows us to compute an importance weight for the email body of new emails based on historical data. \r\n\r\nWith these preparations for our 4 features, we can make our actual ranking calculation for the training data. For this we need to find the ```senderWeight``` (representing the weight of the sender) , ```termWeight``` (representing the weight of the terms in the subject), ```threadGroupWeight``` (representing the thread weight) and ```commonTermsWeight``` (representing the weight of the body of the email) for each email and multiply them to get the final ranking. Due to the fact that we multiply, and not add the values, we need to take care of values that are  ```< 1```. Say for example someone sent 1 email, then the ```senderWeight``` would be 0.69, which would be unfair in comparison to someone who didn't send any email before yet, because he/she would get a ```senderWeight``` of 1. This is why we take the ```Math.max(value,1)``` for each feature that can have values below 1. Let's take a look at the code:\r\n\r\n\r\n```scala\r\n\r\n    val trainingRanks = trainingData.map(mail => {\r\n\r\n      //Determine the weight of the sender, if it is lower than 1, pick 1 instead\r\n      //This is done to prevent the feature from having a negative impact\r\n      val senderWeight = mailsGroupedBySender\r\n        .collectFirst { case (mail.sender, x) =>  Math.max(x,1)}\r\n        .getOrElse(1.0)\r\n\r\n      //Determine the weight of the subject\r\n      val termsInSubject = mail.subject\r\n        .replaceAll(\"[^a-zA-Z ]\", \"\")\r\n        .toLowerCase.split(\" \")\r\n        .filter(x => \tx.nonEmpty &&\r\n        !stopWords.contains(x)\r\n        )\r\n\r\n      val termWeight = if (termsInSubject.size > 0)\r\n        Math.max(termsInSubject\r\n          .map(x => {\r\n          tdm.collectFirst { case (y, z) if y == x => z}\r\n            .getOrElse(1.0)\r\n        })\r\n          .sum / termsInSubject.length,1)\r\n      else 1.0\r\n\r\n      //Determine if the email is from a thread,\r\n      //and if it is the weight from this thread:\r\n      val threadGroupWeight: Double = threadGroupedWithWeights\r\n        .collectFirst { case (mail.subject, _, _, weight) => weight}\r\n        .getOrElse(1.0)\r\n\r\n      //Determine the commonly used terms in the email and the weight belonging to it:\r\n      val termsInMailBody = mail.body\r\n        .replaceAll(\"[^a-zA-Z ]\", \"\")\r\n        .toLowerCase.split(\" \")\r\n        .filter(x => \tx.nonEmpty &&\r\n        !stopWords.contains(x)\r\n        )\r\n\r\n      val commonTermsWeight = if (termsInMailBody.size > 0)\r\n        Math.max(termsInMailBody\r\n          .map(x => {\r\n          tdm.collectFirst { case (y, z) if y == x => z}\r\n            .getOrElse(1.0)\r\n        })\r\n          .sum / termsInMailBody.length,1)\r\n      else 1.0\r\n\r\n      val rank = \ttermWeight *\r\n        threadGroupWeight *\r\n        commonTermsWeight *\r\n        senderWeight\r\n\r\n      (mail, rank)\r\n    })\r\n\r\n    val sortedTrainingRanks = trainingRanks\r\n      .sortBy(x => x._2)\r\n    val median = sortedTrainingRanks(sortedTrainingRanks.length / 2)._2\r\n    val mean = \tsortedTrainingRanks\r\n      .map(x => x._2).sum /\r\n    \t\t\tsortedTrainingRanks.length\r\n```\r\nAs you can see we computed the ranks for all training emails, and additionally sorted them and got the median and the mean. The reason we take this median and mean is to have a *decision boundary* for rating an email as priority or non-priority. In practice, this usually doesn't work out. The best way to actually get this decision boundary is to let the user mark a set of emails as priority vs non priority. You can then use those ranks to calculate the decision boundary, and additionally see if the ranking features are correct. If the user ends up marking emails as non-priority who have a higher ranking than emails that the user marked as priority, you might need to re-evaluate your features. \r\n\r\nThe reason we come up with a decision boundary, rather than just sorting the users email on ranking is due to the time aspect. If you would sort emails purely based on the ranking, one would find this very annoying, because in general people like their email sorted on date. With this decision boundary however, we can mark emails as priority, and show them in a separate list if we were to incorporate our ranking into an email client.\r\n\r\nLet's now look at how many emails from the testing set will be ranked as priority. For this we first have to add the following code:\r\n\r\n```scala\r\n\r\n  val testingRanks = trainingData.map(mail => {\r\n      //mail contains (full content, date, sender, subject, body)\r\n\r\n      //Determine the weight of the sender\r\n      val senderWeight = mailsGroupedBySender\r\n        .collectFirst { case (mail.sender, x) =>  Math.max(x,1)}\r\n        .getOrElse(1.0)\r\n\r\n      //Determine the weight of the subject\r\n      val termsInSubject = mail.subject\r\n        .replaceAll(\"[^a-zA-Z ]\", \"\")\r\n        .toLowerCase.split(\" \")\r\n        .filter(x => \tx.nonEmpty &&\r\n        !stopWords.contains(x)\r\n        )\r\n\r\n      val termWeight = if (termsInSubject.size > 0)\r\n        Math.max(termsInSubject\r\n        .map(x => {\r\n        tdm.collectFirst { case (y, z) if y == x => z}\r\n          .getOrElse(1.0)\r\n      })\r\n        .sum / termsInSubject.length,1)\r\n      else 1.0\r\n\r\n      //Determine if the email is from a thread,\r\n      //and if it is the weight from this thread:\r\n      val threadGroupWeight: Double = threadGroupedWithWeights\r\n        .collectFirst { case (mail.subject, _, _, weight) => weight}\r\n        .getOrElse(1.0)\r\n\r\n      //Determine the commonly used terms in the email and the weight belonging to it:\r\n      val termsInMailBody = mail.body\r\n        .replaceAll(\"[^a-zA-Z ]\", \"\")\r\n        .toLowerCase.split(\" \")\r\n        .filter(x => \tx.nonEmpty &&\r\n        !stopWords.contains(x)\r\n        )\r\n\r\n      val commonTermsWeight = if (termsInMailBody.size > 0)\r\n        Math.max(termsInMailBody\r\n        .map(x => {\r\n        tdm.collectFirst { case (y, z) if y == x => z}\r\n          .getOrElse(1.0)\r\n      })\r\n        .sum / termsInMailBody.length,1)\r\n      else 1.0\r\n\r\n      val rank = \ttermWeight *\r\n        threadGroupWeight *\r\n        commonTermsWeight *\r\n        senderWeight\r\n\r\n      (mail, rank)\r\n    })\r\n\r\n\r\n    val priorityEmails = testingRanks\r\n    \t.filter(x => x._2 >= mean)\r\n\r\n    println(priorityEmails.length + \" ranked as priority\")\r\n\r\n```\r\n\r\nAfter actually running this test code, you will see that the amount of emails ranked as priority from the test set is actually 563. This is 45% of the test email set. This is quite a high value, so we could tweak a bit with the *decision boundary*. However, since we picked this for illustration purposes, and should not be picked to be the mean in practice, we won't look into that percentage any further. Instead we will look at the ranking of the top 10 of the priority emails. \r\n\r\nNote that I've removed part of the email address to prevent spam bots from crawling these mail addresses. We see in the table below that most of these top 10 priority emails are threads grouped together, which had very high activity. Take for example the highest ranked email. This email was a follow up to an email of 9 minutes earlier. This indicates that the email thread was important.  \r\n\r\n| Date | Sender  | Subject  | Rank |\r\n| :--- | :-- | :--  | :-- |\r\n| Sat Sep 07 06:45:23 CEST 2002 | skip@... | [spambayes] can't write to cvs... | 81.11 |\r\n| Sat Sep 07 21:11:36 CEST 2002 | tim.one@... | [spambayes] test sets? | 79.59 |\r\n| Mon Sep 09 17:46:55 CEST 2002 | tim.one@... | [spambayes] deleting \"duplicate\" spam before training?  good idea | 79.54 |\r\n| Mon Aug 26 14:43:00 CEST 2002 | tomwhore@... | how unlucky can you get? | 77.87 |\r\n| Sat Sep 07 06:36:41 CEST 2002 | tim.one@... | [spambayes] can't write to cvs... | 77.77 |\r\n| Sun Sep 08 21:13:40 CEST 2002 | tim.one@... | [spambayes] test sets? | 76.3 |\r\n| Thu Sep 05 20:53:00 CEST 2002 | felicity@... | [razor-users] problem with razor 2.14 and spamassassin 2.41 | 75.44 |\r\n| Fri Sep 06 07:09:11 CEST 2002 | tim.one@... | [spambayes] all but one testing | 72.77 |\r\n| Sat Sep 07 06:40:45 CEST 2002 | tim.one@... | [spambayes] maybe change x-spam-disposition to something else... | 72.62 |\r\n| Sat Sep 07 05:05:45 CEST 2002 | skip@... | [spambayes] maybe change x-spam-disposition to something else... | 72.27 |\r\n\r\nAdditionally we see that ```tim.one...``` occurs a lot in this table.  This indicates that either all his emails are important, or he sent so many emails, that the ranker automatically ranks them as priority.  As final step of this example we will look into this a bit more:\r\n\r\n```scala\r\n\r\nval timsEmails = testingRanks\r\n\t.filter(x => x._1.sender == \"tim.one@...\")\r\n    .sortBy(x => -x._2)\r\ntimsEmails\r\n\t.foreach(x => println(\t\"| \" + \r\n\t\t\t\t\t\t\tx._1.emailDate +\r\n\t\t\t\t\t\t \t\" | \"  + \r\n\t\t\t\t\t\t \tx._1.subject + \r\n\t\t\t\t\t\t \t\" | \" + \r\n\t\t\t\t\t\t \tdf.format(x._2) + \r\n\t\t\t\t\t\t \t\" |\")\r\n\t\t\t\t\t\t )\r\n\r\n```\r\nIf you run this code, a list of 45 emails is printed, and the bottom 10 ranks are as follows:\r\n\r\n\r\n|Date |  Subject  | Rank | \r\n| :--- |  :--  | :-- |  \r\n| Sun Sep 08 21:36:15 CEST 2002 | [spambayes] ditching wordinfo | 42.89 |\r\n| Tue Sep 10 18:12:51 CEST 2002 | [spambayes] timtest broke? | 41.73 |\r\n| Thu Sep 12 04:06:24 CEST 2002 | [spambayes] current histograms | 41.73 |\r\n| Sun Sep 08 21:46:47 CEST 2002 | [spambayes] hammie.py vs. gbayes.py | 41.68 |\r\n| Tue Sep 10 04:18:25 CEST 2002 | [spambayes] current histograms | 40.67 |\r\n| Wed Sep 11 04:46:15 CEST 2002 | [spambayes] xtreme training | 39.83 |\r\n| Tue Sep 10 19:26:05 CEST 2002 | [spambayes] timtest broke? | 39.73 |\r\n| Thu Sep 12 01:37:13 CEST 2002 | [spambayes] stack.pop() ate my multipart message | 38.89 |\r\n| Sat Sep 07 01:06:56 CEST 2002 | [spambayes] ditching wordinfo | 38.34 |\r\n| Sat Sep 07 00:21:15 CEST 2002 | [spambayes] [ann] trained classifier available | 8.71 | \r\n\r\n\r\nIf we recall that our decision boundary was the mean, which is 25.06, then we see only 1 email of Tim did not get ranked as priority. This indicates on one hand that our decision boundary is too low, but on the other hand that Tim might actually be sending a lot of important emails, since the rank can get below the decision boundary. Unfortunately we cannot provide you the exact answer since we are not the owner of this testing data. \r\n\r\nValidating a ranker like this is rather hard when you have no ground truth. One of the most common ways of validating and improving it is by actually presenting it to the user and letting him/her mark correct mistakes. These corrections can then be used to improve the system.\r\n\r\nTo recap, we saw how to get features from raw data, even when the data had 'huge' outliers, and  how to combine these features into a final ranking value. Furthermore we had an attempt at evaluating these features, but due to the lack of knowledge about the dataset we could not draw explicit conclusions. However, if you were to do the same on data you do know, then this should get you started on building your own ranking system. \r\n\r\n##Predicting weight based on height (using Ordinary Least Squares)\r\nIn this section we will introduce the [Ordinary Least Squares](http://en.wikipedia.org/wiki/Ordinary_least_squares) technique which is a form of linear regression. As this technique is quite powerful, it is important to have read [regression](#regression) and the [common pitfalls](#common-pitfalls) before starting with this example. We will cover some of these issues in this section, while others are shown in the sections [underfitting](#underfitting) and [overfitting](#overfitting).\r\n\r\nThe idea behind linear regression is that an 'optimal' regression line is fitted on your training datapoints. Note that this only works if your data is linear, and does not have huge outliers. If this is not the case you could try to manipulate your data until this is the case, for example by taking the ```sqrt``` or ```log``` over the data.\r\n\r\n\r\nAs always, the first thing to do when the project is set-up is to import a dataset. For this we provide you with the following [csv file](./Example%20Data/OLS_Regression_Example_3.csv) and code for reading this file:\r\n\r\n```scala\r\n\r\n  def getDataFromCSV(file: File): (Array[Array[Double]], Array[Double]) = {\r\n    val source = scala.io.Source.fromFile(file)\r\n    val data = source.getLines().drop(1).map(x => getDataFromString(x)).toArray\r\n    source.close()\r\n    var inputData = data.map(x => x._1)\r\n    var resultData = data.map(x => x._2)\r\n\r\n    return (inputData,resultData)\r\n  }\r\n\r\n  def getDataFromString(dataString: String): (Array[Double], Double) = {\r\n\r\n    //Split the comma separated value string into an array of strings\r\n    val dataArray: Array[String] = dataString.split(',')\r\n    var person = 1.0\r\n\r\n    if (dataArray(0) == \"\\\"Male\\\"\") {\r\n      person = 0.0\r\n    }\r\n\r\n    //Extract the values from the strings\r\n    //Since the data is in US metrics \r\n    //inch and pounds we will recalculate this to cm and kilo's\r\n    val data : Array[Double] = Array(person,dataArray(1).toDouble * 2.54)\r\n    val weight: Double = dataArray(2).toDouble * 0.45359237\r\n\r\n    //And return the result in a format that can later easily be used to feed to Smile\r\n    return (data, weight)\r\n  }\r\n\r\n```\r\n\r\nNote that the data reader converts the values from the [Imperial system](http://en.wikipedia.org/wiki/Imperial_units) into the [Metric system](http://en.wikipedia.org/wiki/Metric_system). This has no big effects on the OLS implementation, but since the metric system is [more commonly used](http://www.realclearscience.com/blog/map%20metric%20jpg), we prefer to present the data in that system.\r\n\r\nWith these methods we get the data as an ```Array[Array[Double]]``` which represents the datapoints and an ```Array[Double]``` which represents the classifications as male or female. These formats are good for both plotting the data, and for feeding into the machine learning algorithm.\r\n\r\nLet's first see what the data looks like. For this we plot the data using the following code.\r\n\r\n```scala\r\n\r\nobject LinearRegressionExample extends SimpleSwingApplication {\r\n  def top = new MainFrame {\r\n    title = \"Linear Regression Example\"\r\n    val basePath = \"/Users/.../OLS_Regression_Example_3.csv\"\r\n\r\n    val testData = getDataFromCSV(new File(basePath))\r\n\r\n    val plotData = (testData._1 zip testData._2).map(x => Array(x._1(1) ,x._2))\r\n    val maleFemaleLabels = testData._1.map( x=> x(0).toInt)\r\n    val plot =  ScatterPlot.plot(\tplotData,\r\n    \t\t\t\t\t\t\t\tmaleFemaleLabels,\r\n    \t\t\t\t\t\t\t\t'@',\r\n    \t\t\t\t\t\t\t\tArray(Color.blue, Color.green)\r\n    \t\t\t\t\t\t\t )\r\n    plot.setTitle(\"Weight and heights for male and females\")\r\n    plot.setAxisLabel(0,\"Heights\")\r\n    plot.setAxisLabel(1,\"Weights\")\r\n\r\n    peer.setContentPane(plot)\r\n    size = new Dimension(400, 400)\r\n  }\r\n\r\n```\r\n\r\nIf you execute the code here above, a window will pop up which shows the **right** plot as shown in the image here below. Note that when you run the code, you can scroll to zoom in and out in the plot.\r\n\r\n<img src=\"./Images/HumanDataPoints.png\" width=\"275px\" />\r\n<img src=\"./Images/MaleFemalePlot.png\" width=\"275px\" />\r\n\r\nIn this plot, given that green is female, and blue is male, you can see that there is a big overlap in their weights and heights. So if we were to ignore the difference between male and female it would still look like there was a linear function in the data (which can be seen in the **left** plot). However when ignoring this difference, the function would be not as accurate as it would be when we incorporate the information regarding males and females. \r\n\r\nFinding this distinction is trivial in this example, but you might encounter datasets where these groups are not so obvious. Making you aware of this this possibility might help you find groups in your data, which can improve the performance of your machine learning application.\r\n\r\nNow that we have seen the data and see that indeed we can come up with a linear regression line to fit this data, it is time to train a [model](#model). Smile provides us with the [ordinary least squares](http://en.wikipedia.org/wiki/Ordinary_least_squares) algorithm which we can easily use as follows:\r\n\r\n```scala\r\nval olsModel = new OLS(testData._1,testData._2)\r\n```\r\n\r\nWith this olsModel we can now predict someone's weight based on length and gender as follows: \r\n\r\n```scala\r\nprintln(\"Prediction for Male of 1.7M: \" +olsModel.predict(Array(0.0,170.0)))\r\nprintln(\"Prediction for Female of 1.7M:\" + olsModel.predict(Array(1.0,170.0)))\r\nprintln(\"Model Error:\" + olsModel.error())\r\n```\r\n\r\nand this will give the following results:\r\n\r\n```\r\nPrediction for Male of 1.7M: 79.14538559840447\r\nPrediction for Female of 1.7M:70.35580395758966\r\nModel Error:4.5423150758157185\r\n```\r\n\r\nIf you recall from the classification algorithms, there was a [prior](#prior) value to be able to say something about the performance of your model. Since regression is a stronger statistical method, you have an actual error value now. This value represents how far off the fitted regression line is in average, such that you can say that for this model, the prediction for a male of 1.70m is 79.15kg  +/- the error of 4.54kg. Note that if you would remove the distinction between males and females, this error would increase to 5.5428. In other words, adding the distinction between male and female, increases the model accuracy by +/- 1 kg in its predictions.\r\n\r\nFinally Smile also provides you with some statistical information regarding your model. The method ```RSquared``` gives you the [root-mean-square error (RMSE)](#root-mean-squared-error-rmse) from the model divided by the [RMSE](#root-mean-squared-error-rmse) from the mean. This value will always be between 0 and 1. If your model predicts every datapoint perfectly, RSquared will be 1, and if the model does not perform better than the mean function, the value will be 0. In the field this measure is often multiplied by 100 and then used as representation of how accurate the model is. Because this is a normalised value, it can be used to compare the performance of different models.\r\n\r\nThis concludes linear regression, if you want to know more about how to apply regression on  non-linear data, feel free to work through the next example [An attempt at rank prediction for top selling books using text regression](#an-attempt-at-rank-prediction-for-top-selling-books-using-text-regression).\r\n\r\n##An attempt at rank prediction for top selling books using text regression\r\n\r\nIn the example of [predicting weights based on heights and gender](#predicting-weight-based-on-height-using-ordinary-least-squares) we introduced the notion of linear regression. However, sometimes one would want to apply regression on non numeric data such as text. \r\n\r\nIn this example we will show how text regression can be done by an attempt at predicting the top 100 selling books from O'Reilly. Additionally with this example we will also show that for this particular case it does not work out to use text regression. The reason for this is that the data simply does not contain a signal for our test data. However this does not make this example useless because there might be an actual signal within the data you are using in practice, which then can be detected using text regression as explained here.\r\n\r\nThe data file we used in this example can be downloaded [here](./Example%20Data/TextRegression_Example_1.csv). Next to the Smile library, in this example we will also use the [Scala-csv library](https://www.versioneye.com/java/com.github.tototoshi:scala-csv_2.11/1.2.1) as the csv contains strings with comma's in them. Let's start off with getting the data we need:\r\n\r\n```scala\r\n\r\nobject TextRegression  {\r\n\r\n  def main(args: Array[String]): Unit = {\r\n\r\n    //Get the example data\r\n      val basePath = \"/users/.../TextRegression_Example_4.csv\"\r\n      val testData = getDataFromCSV(new File(basePath))\r\n  }\r\n\r\n  def getDataFromCSV(file: File) : List[(String,Int,String)]= {\r\n    val reader = CSVReader.open(file)\r\n    val data = reader.all()\r\n\r\n    val documents = data.drop(1).map(x => (x(1),x(3)toInt,x(4)))\r\n    return documents\r\n  }\r\n}\r\n\r\n```\r\n\r\nWe now have the title, rank and long description of the top 100 selling books from O'Reilly. However when we want to do regression of some form, we need numeric data. This is why we will build a [Document Term Matrix (DTM)](http://en.wikipedia.org/wiki/Document-term_matrix). Note that this DTM is similar to the Term Document Matrix (TDM) that we built in the spam classification example. Its difference is that we store document records containing which terms are in that document, in contrast to the TDM where we store records of words, containing a list of documents in which this term is available.\r\n\r\nWe implemented the DTM ourselves as follows:\r\n\r\n```scala\r\nimport java.io.File\r\nimport scala.collection.mutable\r\n\r\nclass DTM {\r\n\r\n  var records: List[DTMRecord] = List[DTMRecord]()\r\n  var wordList: List[String] = List[String]()\r\n\r\n  def addDocumentToRecords(documentName: String, rank: Int, documentContent: String) = {\r\n    //Find a record for the document\r\n    val record = records.find(x => x.document == documentName)\r\n    if (record.nonEmpty) {\r\n      throw new Exception(\"Document already exists in the records\")\r\n    }\r\n\r\n    var wordRecords = mutable.HashMap[String, Int]()\r\n    val individualWords = documentContent.toLowerCase.split(\" \")\r\n    individualWords.foreach { x =>\r\n      val wordRecord = wordRecords.find(y => y._1 == x)\r\n      if (wordRecord.nonEmpty) {\r\n        wordRecords += x -> (wordRecord.get._2 + 1)\r\n      }\r\n      else {\r\n        wordRecords += x -> 1\r\n        wordList = x :: wordList\r\n      }\r\n    }\r\n    records = new DTMRecord(documentName, rank, wordRecords) :: records\r\n  }\r\n\r\n  def getStopWords(): List[String] = {\r\n    val source = scala.io.Source.fromFile(new File(\"/Users/.../stopwords.txt\"))(\"latin1\")\r\n    val lines = source.mkString.split(\"\\n\")\r\n    source.close()\r\n    return lines.toList\r\n  }\r\n\r\n  def getNumericRepresentationForRecords(): (Array[Array[Double]], Array[Double]) = {\r\n    //First filter out all stop words:\r\n    val StopWords = getStopWords()\r\n    wordList = wordList.filter(x => !StopWords.contains(x))\r\n    \r\n    var dtmNumeric = Array[Array[Double]]()\r\n    var ranks = Array[Double]()\r\n\r\n    records.foreach { x =>\r\n      //Add the rank to the array of ranks\r\n      ranks = ranks :+ x.rank.toDouble\r\n\r\n      //And create an array representing all words and their occurrences \r\n      //for this document:\r\n      var dtmNumericRecord: Array[Double] = Array()\r\n      wordList.foreach { y =>\r\n\r\n        val termRecord = x.occurrences.find(z => z._1 == y)\r\n        if (termRecord.nonEmpty) {\r\n          dtmNumericRecord = dtmNumericRecord :+ termRecord.get._2.toDouble\r\n        }\r\n        else {\r\n          dtmNumericRecord = dtmNumericRecord :+ 0.0\r\n        }\r\n      }\r\n      dtmNumeric = dtmNumeric :+ dtmNumericRecord\r\n\r\n    }\r\n\r\n    return (dtmNumeric, ranks)\r\n  }\r\n}\r\n\r\nclass DTMRecord(val document : String, \r\n\t\t\t\tval rank : Int, \r\n\t\t\t\tvar occurrences :  mutable.HashMap[String,Int] \r\n\t\t\t\t)\r\n\r\n```\r\n\r\nIf you look at this implementation you'll notice that there is a method called  ```def getNumericRepresentationForRecords(): (Array[Array[Double]], Array[Double])```. This method returns a tuple with as first parameter a matrix in which each row represents a document, and each column represents one of the words from the complete vocabulary of the DTM's documents. Note that the doubles in the first table represent the number of occurrences of the words.\r\n\r\nThe second parameter is an array containing all the ranks belonging to the records from the first table. \r\n\r\nWe can now extend our main code such that we get the numeric representation of all the documents as follows:\r\n\r\n```scala\r\n\r\nval documentTermMatrix  = new DTM()\r\ntestData.foreach(x => documentTermMatrix.addDocumentToRecords(x._1,x._2,x._3))\r\n\r\n```\r\n\r\nWith this conversion from text to numeric value's we can open our regression toolbox. We used [Ordinary Least Squares(OLS)](http://en.wikipedia.org/wiki/Ordinary_least_squares) in the example [predicting weight based on height](#predicting-weight-based-on-height-using-ordinary-least-squares), however this time we will use [Least Absolute Shrinkage and Selection Operator (Lasso)](http://en.wikipedia.org/wiki/Least_squares#Lasso_method) regression. This is because we can give this regression method a certain lambda, which represents a penalty value. This penalty value allows the LASSO algorithm to select relevant features (words) while discarding some of the other features (words). \r\n\r\nThis feature selection that Lasso performs is very useful in our case due too the large set of words that is used in the documents descriptions. Lasso will try to come up with an ideal subset of those words as features, whereas when applying the OLS, all words would be used, and the runtime would be extremely high. Additionally, the OLS implementation of Smile detects rank deficiency. This is one of the [curses of dimensionality](#curse-of-dimensionality)\r\n\r\nWe need to find an optimal lambda however, thus we should try for several lambda's using cross validation. We will do this as follows:\r\n\r\n```scala\r\n\r\n    for (i <- 0 until cv.k) {\r\n      //Split off the training datapoints and classifiers from the dataset\r\n      val dpForTraining = numericDTM\r\n      \t._1\r\n      \t.zipWithIndex\r\n      \t.filter(x => cv\r\n      \t\t\t\t.test(i)\r\n      \t\t\t\t.toList\r\n      \t\t\t\t.contains(x._2)\r\n      \t\t\t)\r\n      \t.map(y => y._1)\r\n      \t\r\n      val classifiersForTraining = numericDTM\r\n      \t._2\r\n      \t.zipWithIndex\r\n      \t.filter(x => cv\r\n      \t\t\t\t.test(i)\r\n      \t\t\t\t.toList\r\n      \t\t\t\t.contains(x._2)\r\n      \t\t\t)\r\n      \t.map(y => y._1)\r\n\r\n      //And the corresponding subset of data points and their classifiers for testing\r\n      val dpForTesting = numericDTM\r\n      \t._1\r\n      \t.zipWithIndex\r\n      \t.filter(x => !cv\r\n      \t\t\t\t.test(i)\r\n      \t\t\t\t.contains(x._2)\r\n      \t\t\t)\r\n      \t.map(y => y._1)\r\n      \t\r\n      val classifiersForTesting = numericDTM\r\n      \t._2\r\n      \t.zipWithIndex\r\n      \t.filter(x => !cv\r\n      \t\t\t\t.test(i)\r\n      \t\t\t\t.contains(x._2)\r\n      \t\t\t)\r\n      \t.map(y => y._1)\r\n\r\n      //These are the lambda values we will verify against\r\n      val lambdas: Array[Double] = Array(0.1, 0.25, 0.5, 1.0, 2.0, 5.0)\r\n\r\n      lambdas.foreach { x =>\r\n        //Define a new model based on the training data and one of the lambda's\r\n        val model = new LASSO(dpForTraining, classifiersForTraining, x)\r\n\r\n        //Compute the RMSE for this model with this lambda\r\n        val results = dpForTesting.map(y => model.predict(y)) zip classifiersForTesting\r\n        val RMSE = Math\r\n        \t.sqrt(results\r\n        \t\t\t.map(x => Math.pow(x._1 - x._2, 2)).sum /\r\n        \t\t\t\t\t\t results.length\r\n        \t\t\t\t)\r\n        println(\"Lambda: \" + x + \" RMSE: \" + RMSE)\r\n\r\n      }\r\n    }\r\n    \r\n```\r\n\r\nRunning this code multiple times gives an [RMSE](#root-mean-squared-error) varying between 36 and 51. This means that the rank prediction we would do would be off by at least 36 ranks. Given the fact that we are trying to predict the top 100 ranks, it shows that the algorithm performs very poorly. Differences in lambda are in this case not noticeable. However when using this in practice you should be careful when picking the lambda value: The higher the lambda you pick, the lower the amount of features for the algorithm becomes. This is why cross validation is important to see how the algorithm performs on different lambda's.\r\n\r\nTo conclude this example, we rephrase a quote from [John Tukey](http://en.wikipedia.org/wiki/John_Tukey): \r\n>The data may not contain the answer. The combination of some data and an aching desire for an answer does not ensure that a reasonable answer can be extracted from a given body of data.\r\n\r\n##Using unsupervised learning to merge features (PCA)\r\n\r\nThe basic idea of [PCA](#principal-components-analysis-pca) is to reduce the amount of dimensions of a problem. This can be useful for getting rid of the [curse of dimensionality](#curse-of-dimensionality) or to merge data such that you can see trends within the data without noise of correlated data. \r\n\r\nIn this example we are going to use [PCA](#principal-components-analysis-pca) to merge stock prices from 24 stocks into 1 over a time period of 2002 - 2012. This single value (over time) then represents a stock market index based on data of these 24 stocks. Merging these 24 stock prices into 1 significantly reduces the amount of data to process, and decreases the dimension of our data, which is a big advantage if we later apply other machine learning algorithms such as regression for prediction. In order to see the performance of our feature reduction of 24 to 1, we will compare our result to the Dow Jones Index (DJI) over that same time period. \r\n\r\nWith the project set-up the next step is to load the data. For this we provide you with 2 files: [Data file 1](./Example%20Data/PCA_Example_1.csv) and [Data file 2](./Example%20Data/PCA_Example_2.csv).\r\n\r\n```scala\r\nobject PCA extends SimpleSwingApplication{\r\n\r\n\r\n  def top = new MainFrame {\r\n    title = \"PCA Example\"\r\n    //Get the example data\r\n    val basePath = \"/users/.../Example Data/\"\r\n    val exampleDataPath = basePath + \"PCA_Example_1.csv\"\r\n    val trainData = getStockDataFromCSV(new File(exampleDataPath))\r\n    \r\n    }\r\n  def getStockDataFromCSV(file: File): (Array[Date],Array[Array[Double]]) = {\r\n    val source = scala.io.Source.fromFile(file)\r\n    //Get all the records (minus the header)\r\n    val data = source\r\n    \t.getLines()\r\n    \t.drop(1)\r\n    \t.map(x => getStockDataFromString(x))\r\n    \t.toArray\r\n    \t\r\n    source.close()\r\n    //group all records by date, and sort the groups on date ascending\r\n    val groupedByDate = data.groupBy(x => x._1).toArray.sortBy(x => x._1)\r\n    //extract the values from the 3-tuple and turn them into\r\n    // an array of tuples: Array[(Date, Array[Double)]\r\n    val dateArrayTuples = groupedByDate\r\n    \t.map(x => (x._1, x\r\n    \t\t\t\t\t._2\r\n    \t\t\t\t\t.sortBy(x => x._2)\r\n    \t\t\t\t\t.map(y => y._3)\r\n    \t\t\t\t)\r\n    \t\t)\r\n\r\n    //turn the tuples into two separate arrays for easier use later on\r\n    val dateArray = dateArrayTuples.map(x => x._1).toArray\r\n    val doubleArray = dateArrayTuples.map(x => x._2).toArray\r\n\r\n\r\n    (dateArray,doubleArray)\r\n  }\r\n\r\n  def getStockDataFromString(dataString: String): (Date,String,Double) = {\r\n\r\n    //Split the comma separated value string into an array of strings\r\n    val dataArray: Array[String] = dataString.split(',')\r\n\r\n    val format = new SimpleDateFormat(\"yyyy-MM-dd\")\r\n    //Extract the values from the strings\r\n\r\n    val date = format.parse(dataArray(0))\r\n    val stock: String = dataArray(1)\r\n    val close: Double = dataArray(2).toDouble\r\n\r\n    //And return the result in a format that can later \r\n    //easily be used to feed to Smile\r\n    \r\n    (date,stock,close)\r\n  }\r\n}\r\n```\r\n\r\nWith this training data, and the fact that we already know that we want to merge the 24 features into 1 single feature, we can do the PCA and retrieve the values for the datapoints as follows.\r\n\r\n```scala\r\n\r\n//Add to `def top`\r\nval pca = new PCA(trainData._2)\r\npca.setProjection(1)\r\nval points = pca.project(trainData._2)\r\nval plotData = points\r\n\t.zipWithIndex\r\n\t.map(x => Array(x._2.toDouble, -x._1(0) ))\r\n\t\r\nval canvas: PlotCanvas = LinePlot.plot(\"Merged Features Index\",\r\n\t\t\t\t\t\t\t\t\t\t plotData, \r\n\t\t\t\t\t\t\t\t\t\t Line.Style.DASH, \r\n\t\t\t\t\t\t\t\t\t\t Color.RED);\r\n\r\npeer.setContentPane(canvas)\r\nsize = new Dimension(400, 400)\r\n\r\n```\r\n\r\nThis code not only does the PCA but also plots the results, with the feature value on the y axis and the individual days on the x axis.\r\n\r\n<img src=\"./Images/Unscaled_PCA_Index.png\" width=\"400px\" />\r\n\r\nIn order to be able to see how our PCA combination words, we should now add the Dow Jones index to the plot by adjusting the code as follows:\r\n\r\n\r\nFirst we add this to the ```def top``` method\r\n```scala\r\n \r\n //Verification against DJI\r\n    val verificationDataPath = basePath + \"PCA_Example_2.csv\"\r\n    val verificationData = getDJIFromFile(new File(verificationDataPath))\r\n    val DJIIndex = getDJIFromFile(new File(verificationDataPath))\r\n    canvas.line(\"Dow Jones Index\", DJIIndex._2, Line.Style.DOT_DASH, Color.BLUE)\r\n\r\n```\r\n\r\nAnd then we need to introduce the following two methods:\r\n\r\n```scala\r\n\r\n  def getDJIRecordFromString(dataString: String): (Date,Double) = {\r\n\r\n    //Split the comma separated value string into an array of strings\r\n    val dataArray: Array[String] = dataString.split(',')\r\n\r\n    val format = new SimpleDateFormat(\"yyyy-MM-dd\")\r\n    //Extract the values from the strings\r\n\r\n    val date = format.parse(dataArray(0))\r\n    val close: Double = dataArray(4).toDouble\r\n\r\n    //And return the result in a format that can later \r\n    //easily be used to feed to Smile\r\n    (date,close)\r\n  }\r\n\r\n  def getDJIFromFile(file: File): (Array[Date],Array[Double]) = {\r\n    val source = scala.io.Source.fromFile(file)\r\n    //Get all the records (minus the header)\r\n    val data = source\r\n    \t.getLines()\r\n    \t.drop(1)\r\n    \t.map(x => getDJIRecordFromString(x)).toArray\r\n    source.close()\r\n\r\n    //turn the tuples into two separate arrays for easier use later on\r\n    val sortedData = data.sortBy(x => x._1)\r\n    val dates = sortedData.map(x => x._1)\r\n    val doubles = sortedData.map(x =>   x._2 )\r\n\r\n    (dates, doubles)\r\n  }\r\n\r\n```\r\nThis code loads the DJI data, and adds it in the plot which already contained our own index of the stock market. However, when we execute this code, the result plot is rather useless.\r\n\r\n<img src=\"./Images/Unscaled_DJI_PCA_Index.png\" width=\"400px\" />\r\n\r\nAs you can see, the ranges of the DJI and our computed feature are far off. This is why we will now normalise the data. The idea is that we scale the data based on its range, such that both datasets are on the same scale.\r\n\r\nReplace the ```getDJIFromFile``` method with the following:\r\n\r\n```scala\r\n\r\ndef getDJIFromFile(file: File): (Array[Date],Array[Double]) = {\r\n    val source = scala.io.Source.fromFile(file)\r\n    //Get all the records (minus the header)\r\n    val data = source\r\n    \t.getLines()\r\n    \t.drop(1)\r\n    \t.map(x => getDJIRecordFromString(x))\r\n    \t.toArray\r\n    \t\r\n    source.close()\r\n\r\n    //turn the tuples into two separate arrays for easier use later on\r\n    val sortedData = data.sortBy(x => x._1)\r\n    val dates = sortedData.map(x => x._1)\r\n    val maxDouble = sortedData.maxBy(x => x._2)._2\r\n    val minDouble = sortedData.minBy(x => x._2)._2\r\n    val rangeValue = maxDouble - minDouble\r\n    val doubles = sortedData.map(x =>   x._2 / rangeValue )\r\n\r\n    (dates, doubles)\r\n  }\r\n  \r\n```\r\nand replace the `plotData` definition in the method```def top``` with\r\n\r\n```scala\r\nval maxDataValue = points.maxBy(x => x(0))\r\nval minDataValue = points.minBy(x => x(0))\r\nval rangeValue = maxDataValue(0) - minDataValue(0)\r\nval plotData = points\r\n\t.zipWithIndex\r\n\t.map(x => Array(x._2.toDouble, -x._1(0) / rangeValue))\r\n```\r\n\r\n<img src=\"./Images/PCA_Normalised.png\" width=\"400px\" />\r\n\r\nWe see now that even though the data of the DJI ranges between 0.8 and 1.8 whereas our new feature ranges between -0.5 and 0.5, the trend lines correspond quite well. With this example, and the explanation of [PCA](#principal-components-analysis-pca) in the general section you should now be able to use PCA and apply it to your own data.\r\n\r\n##Using Support Vector Machines (SVM's)\r\n\r\nBefore we actually go into using Support Vector Machines (SVM's)  I'll first mildly introduce what an SVM is. The basic SVM is a binary classifier that divides a dataset into 2 parts by picking a hyperplane that represents the largest separation between the datapoints. A SVM takes a so called 'correction rate' value. If there is no perfect split, the correction rate allows for picking a hyperplane that still splits as well as possible within that error rate. Thus the correction rate allows the hyperplane to be fit even when there are some points in the way. This means that we cannot come up with a 'standard' correction rate for every case. However when there is no overlap in the data, lower values should perform better than higher values. \r\n\r\nI just explained the basic SVM, which is a binary classifier, but this same idea can be used with more classes as well. However, for now we will stick with 2 classes, as there is enough to address already with just 2 classes.\r\n\r\nIn this example we will work through several small cases where a Support Vector Machine (SVM) will outperform other classifying algorithms such as [KNN](#labeling-isps-based-on-their-downupload-speed-knn-using-smile-in-scala). This approach is different from the former examples, but will help you understand how and when to use SVM's more easily. \r\n\r\nFor each sub example we will provide code, a plot, a few test runs with different parameters on the SVM and an explanation on the results. This should give you an idea on the parameters to feed into the SVM algorithm. \r\n\r\n\r\nIn the first example we will use the ```GaussianKernel```, but there are many other kernels available in [Smile](https://github.com/haifengl/smile). The other kernels can be found [here](http://haifengl.github.io/smile/doc/smile/math/kernel/MercerKernel.html). Next to the ```GaussianKernel``` we will address the ```PolynomialKernel``` as this one differs a lot from the Gaussian one.\r\n\r\nWe will use the following base for each example, with only the  ```filePaths``` and ```svm``` construction changing per example.\r\n\r\n```scala\r\n\r\nobject SupportVectorMachine extends SimpleSwingApplication {\r\n\r\n\r\n  def top = new MainFrame {\r\n    title = \"SVM Examples\"\r\n    //File path (this changes per example)\r\n    val trainingPath =  \"/users/.../Example Data/SVM_Example_1.csv\"\r\n\tval testingPath =  \"/users/.../Example Data/SVM_Example_1.csv\"\r\n    //Loading of the test data and plot generation stays the same\r\n    val trainingData = getDataFromCSV(new File(path))\r\n    val testingData = getDataFromCSV(new File(path))\r\n    \r\n    val plot = ScatterPlot.plot(\ttrainingData._1, \r\n    \t\t\t\t\t\t\t\ttrainingData._2, \r\n    \t\t\t\t\t\t\t\t'@', \r\n    \t\t\t\t\t\t\t\tArray(Color.blue, Color.green)\r\n    \t\t\t\t\t\t\t)\r\n    peer.setContentPane(plot)\r\n\r\n    //Here we do our SVM fine tuning with possibly different kernels\r\n    val svm = new SVM[Array[Double]](new GaussianKernel(0.01), 1.0,2)\r\n    svm.learn(trainingData._1, trainingData._2)\r\n    svm.finish()\r\n\r\n    //Calculate how well the SVM predicts on the training set\r\n    val predictions = testingData\r\n    \t._1\r\n    \t.map(x => svm.predict(x))\r\n    \t.zip(testingData._2)\r\n    \t\r\n    val falsePredictions = predictions\r\n    \t.map(x => if (x._1 == x._2) 0 else 1 )\r\n\r\n    println(falsePredictions.sum.toDouble / predictions.length  \r\n    \t\t\t\t* 100 + \" % false predicted\")\r\n\r\n    size = new Dimension(400, 400)\r\n  }\r\n\r\n\r\n  def getDataFromCSV(file: File): (Array[Array[Double]], Array[Int]) = {\r\n    val source = scala.io.Source.fromFile(file)\r\n    val data = source\r\n    \t.getLines()\r\n    \t.drop(1)\r\n    \t.map(x => getDataFromString(x))\r\n    \t.toArray\r\n    \t\r\n    source.close()\r\n    val dataPoints = data.map(x => x._1)\r\n    val classifierArray = data.map(x => x._2)\r\n    return (dataPoints, classifierArray)\r\n  }\r\n\r\n  def getDataFromString(dataString: String): (Array[Double], Int) = {\r\n    //Split the comma separated value string into an array of strings\r\n    val dataArray: Array[String] = dataString.split(',')\r\n\r\n    //Extract the values from the strings\r\n    val coordinates  = Array( dataArray(0).toDouble, dataArray(1).toDouble)\r\n    val classifier: Int = dataArray(2).toInt\r\n\r\n    //And return the result in a format that can later \r\n    //easily be used to feed to Smile\r\n    return (coordinates, classifier)\r\n  }\r\n  \r\n```\r\n\r\n###Example 1 (Gaussian Kernel)\r\nIn this example we present the most commonly used kernel for SVMs, namely the Gaussian Kernel. The idea behind this example is to help finding good input parameters on this kernel. The data we used for this example can be downloaded [here](./Example%20Data/SVM_Example_1.csv).\r\n\r\n<img src=\"./Images/SVM_Datapoints.png\" width=\"400px\" />\r\n\r\nIt is clear from this plot that a linear regression line would not work. Instead we will use a SVM to make predictions. In the first code we gave, the ```GaussianKernel``` with a sigma of 0.01, a margin penalty of 1.0 and the amount of classes of 2 is passed to SVM. Now what does this all mean?\r\n\r\nLet's start with the ```GaussianKernel```. This kernel represents the way in which the SVM will calculate the similarity over pairs of datapoints in the system. For the ```GaussianKernel``` the variance in the euclidian distance is used. The reason for picking the GaussianKernel specifically is because the data does not contain a clear structure such as a linear, polynomial or hyperbolic function. Instead the data is clustered in 3 groups. \r\n\r\nThe parameter we pass in the constructor of the ```GaussianKernel``` is the sigma. This sigma value represents a smoothness value of the kernel. We will show how changing this parameter affects the predictions. As margin penalty we pass 1. This parameter defines the margin of the vectors in the system, thus making this value lower results in more bounded vectors. We will show with a set of runs and their results what kind of effect this has in practice. Note that the `s:` stands for sigma, and the `c:` stands for the correction penalty. The percentages represent the error rate in the prediction, which is simply the percentage of false predictions on the same dataset after training.\r\n\r\n| c, s-> | 0.001 |  0.01 |  0.1 |  0.2 | 0.5 | 1.0 |  2.0 |  3.0 |  10.0 |\r\n| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: |\r\n| **0.001** | 48.4% | 48.4% | 48.4% | 48.4% | 48.4% | 48.4% | 48.4% | 48.4% | 48.4% | \r\n|**0.01** | 48.4% | 48.4% | 40% | 43.8% | 48.4% | 48.4% | 48.4% | 48.4% | 48.4% |\r\n| **0.1** | 48.4% | 48.4% | 12.4% | 14.2% | 17.4% | 48.4% | 48.4% | 48.4% | 48.4% | \r\n| **0.2** | 48.4% | 45.6% | 9.1% | 10.1% | 12.3% | 48.4% | 48.4% | 48.4% | 48.4% |\r\n| **0.5** | 47.5% | 3.4% | 6.3% | 7.2% | 7% | 8.9% | 48.4% | 48.4% | 48.4% | \r\n| **1.0** | 0% | 1.6% | 5.1% | 5.7% | 5.6% | 5.6% | 48.4% | 48.4% | 48.4% |\r\n| **2.0** | 0% | 1% | 5.2% | 5% | 5.4% | 5.7% | 13.1% | 48.4% | 51.6% | |\r\n| **3.0** | 0% | 1.2% | 6.4% | 5.8% | 5.7% | 7.4% | 18% | 51.6% | 51.6% |\r\n| **10.0** | 0% | 1.5% | 7.5% | 6.4% | 7.7% | 12.9% | 26.2% | 51.6% | 51.6% |\r\n| **100.0** | 0% | 1.5% | 10.1% | 12.8% | 14.6% | 18.3% | 41.6% | 51.6% | 51.6% |\r\n\r\nUnfortunately there is no golden rule for finding the right sigma for every dataset. Possibly one of the best approaches is to calculate the sigma for your data, which is the `√(variance)` and then take steps around that value to see which sigma performs well. Since the variance in this data was between 0.2 and 0.5 we took this as center and explored several values at each side of this center to see the performance of the SVM with the gaussian kernel in our case.\r\n \r\nWhen we look at the results table and their false prediction percentages, it shows that the best performance is with a very low sigma (0.001) and a correction rate of 1.0 and up. However if we would use this model in practice with new data, it might be [overfitted](#overfitting). This is why you should always be careful when testing the model against its own training data. A better approach would be to perform [cross validation](#cross-validation), or verify against future data.\r\n\r\n###Example 2 (Polynomial Kernel)\r\n\r\nThe gaussian kernel is not always the best choice, even though it is the most commonly picked kernel when using SVM's. This is why in this example we will show a case in which a polynomial kernel outperforms the gaussian kernel. Note that even though the example data for this case is constructed, similar data can be found in the field (with a bit more noise). The training data we used for this example can be downloaded [here](./Example%20Data/SVM_Example_2.csv), and the testing data [here](./Example%20Data/SVM_Example_2_Test_data.csv)\r\n\r\nFor the example data we created 2 classes with a polynomial degree of 3, and generated a testing and training data file. The training data contains the first 500 points on the x axis, whereas the testing data contains the points from 500 to 1000 on the x axis. In order to see why a polynomial kernel would work, we plot the data. The left plot is the training data, and the right one the testing data.\r\n\r\n<img src=\"./Images/SVM_TrainData.png\" width=\"325px\" />\r\n<img src=\"./Images/SVM_TestData.png\" width=\"325px\" />\r\n\r\nGiven the base code at the start of this practical example we do the following replacements:\r\n\r\n```scala\r\nval trainingPath = \"/users/.../Example Data/SVM_Example_2.csv\"\r\nval testingPath = \"/users/.../Example Data/SVM_Example_2_Test_data.csv\"\r\n\r\n```\r\n\r\nIf we then run the code to see the performance with the gaussian kernel we get the following results:\r\n\r\n| c, s-> | 0.01 | 0.1 | 0.2 | 0.5 | 1.0 | 2.0 | 3.0 | 10.0 | 100.0 |\r\n| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | \r\n| **0.001** | 50% | 50% | 50% | 50% | 50% | 50% | 50% | 50% | 50% |\r\n| **0.01** | 50% | 50% | 50% | 50% | 50% | 50% | 50% | 50% | 39.8% |\r\n| **0.1** | 50% | 50% | 50% | 50% | 50% | 49.4% | 49.1% | 47.5% | 35.4% |\r\n| **0.2** | 50% | 50% | 50% | 50% | 49.4% | 49.2% | 48.8% | 47.1% | 34.3% |\r\n| **0.5** | 50% | 49.9% | 49.8% | 49.5% | 49.3% | 48.9% | 48.6% | 46.8% | 33.1% |\r\n| **1.0** | 50% | 49.8% | 49.7% | 49.4% | 49.4% | 48.9% | 48.6% | 46.2% | 50% |\r\n| **2.0** | 50% | 49.8% | 49.7% | 49.4% | 49.3% | 49.2% | 48.8% | 45.7% | 31.7% |\r\n| **3.0** | 50% | 49.8% | 49.7% | 49.8% | 49.5% | 49% | 48.8% | 46.2% | 27.4% |\r\n| **10.0** | 50% | 49.8% | 49.7% | 49.4% | 49.3% | 49% | 48.4% | 46.7% | 33.2% |\r\n| **100.0** | 50% | 49.8% | 49.7% | 49.4% | 49.3% | 49.1% | 48.6% | 46.7% | 32.2% |\r\n\r\nWe see that even in the best case, still 27.4% of the testing data is falsely classified. This is interesting as when we look at the plots, a very clear distinction can be found between both classes. We could fine tune the sigma and correction rate, but when prediction points very far away (say x is 100000) the sigma and correction rate would be way to high for it to do a good performance (time wise and prediction wise).\r\n\r\nSo let's change the kernel from gaussian to a polynomial one as follows\r\n\r\n```scala\r\n\r\n val svm = new SVM[Array[Double]](new PolynomialKernel(2), 1.0,2)\r\n \r\n```\r\nNote how we pass 2 in the constructor of the ```PolynomialKernel```. This 2 represents the degree of the function it tries to fit on. If we perform the runs not only for degree 2, but for degrees of 2,3,4,5 and for the correction rates differing again from 0.001 up to 100, we get the following results:\r\n\r\n| | degree: 2 | degree: 3 | degree: 4 | degree: 5 |\r\n| :-- | :--: | :--: | :--: | :--: | :--: | :--: | :--: | \r\n| **c: 0.001** | 49.7% | 0% | 49.8% | 0% |\r\n| **c: 0.01** | 49.8% | 0% | 49.8% | 0% |\r\n| **c: 0.1** | 49.8% | 0% | 49.8% | 0% |\r\n| **c: 0.2** | 49.8% | 0% | 49.8% | 0% |\r\n| **c: 0.5** | 49.8% | 0% | 50% | 0% |\r\n| **c: 1.0** | 49.9% | 0% | 50% | 0% |\r\n| **c: 2.0** | 50% | 0% | 47.9% | 0% |\r\n| **c: 3.0** | 38.4% | 0% | 50% | 0% |\r\n| **c: 10.0** | 49.5% | 0% | 49.5% | 0% |\r\n| **c: 100.0** | 49.5% | 0% | 49.5% | 0% |\r\n\r\nHere we see that the testing data, which did not contain 1 single overlapping point with the training data, gets 100% accuracy for degrees 3 and 5. This is an amazing performance in comparison to the gaussian kernel with 27.4% in its best case. Do note that this example data is constructed, thus did not contain a lot of noise. This is why the error rate is 0% for all 'correction rates'. If you would add noise, fine-tuning of this correction rate would be needed.\r\n\r\nThis concludes the section on Support Vector machines. \r\n\r\n#Conclusion\r\n\r\nAfter reading the global idea of machine learning you should be able to identify your case as a [classification](#classification), [regression](#regression) or [dimension reduction](#principal-components-analysis-pca) problem. Additionally you should understand the basic concept of machine learning, what a [model](#model) is, and you should be aware of some of the [common pitfalls](#common-pitfalls) in machine learning.\r\n\r\n\r\nAfter working through the practical examples on this blog you should be able to use [K-NN](#labeling-isps-based-on-their-downupload-speed-k-nn-using-smile-in-scala), [Naive Bayes](#classifying-email-as-spam-or-ham-naive-bayes), and [linear regression](#predicting-weight-based-on-height-using-ordinary-least-squares). Additionally you should be able to perform [text regression](#an-attempt-at-rank-prediction-for-top-selling-books-using-text-regression), merge features using [PCA](#using-unsupervised-learning-to-merge-features-pca) and use [Support Vector Machines](#using-support-vector-machines-svms), and last but not least build your own [recommendation system](#ranking-emails-based-on-their-content-recommendation-system).\r\n\r\nIn case you have questions or feedback regarding this blog, feel free to contact me via [Github](https://github.com/Xyclade), [LinkedIn](https://www.linkedin.com/profile/public-profile-settings?trk=prof-edit-edit-public_profile) or [Twitter](https://twitter.com/Mikedewaard).\r\n\r\n\r\n\r\n\r\n\r\n","google":"UA-60800942-1","note":"Don't delete this file! It's used internally to help with page regeneration."}